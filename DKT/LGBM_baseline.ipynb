{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "import missingno as msno\n",
    "import os\n",
    "from data_loader import FeatureEngineering\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 80 ms, total: 4.2 s\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtype = {\n",
    "    'userID': 'int16',\n",
    "    'answerCode': 'int8',\n",
    "    'KnowledgeTag': 'int16'\n",
    "}   \n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "df = df.sort_values(by=['userID', 'Timestamp', 'testId']).reset_index(drop=True)\n",
    "copy_df = df.copy()\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def feature_engineering(df):\n",
    "    # 문제별 풀이시간\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['diff_Timestamp'] = df['Timestamp'] - df.shift(1)['Timestamp']\n",
    "\n",
    "    testId_df = df[~df.duplicated(['assessmentItemID'])].groupby('testId')\n",
    "    testId2len = {}\n",
    "    for testId, g_df in testId_df:\n",
    "        testId2len[testId] = len(g_df)\n",
    "\n",
    "    userID_df = df.groupby('userID')\n",
    "    start_index_list = []\n",
    "    second_index_list = []\n",
    "\n",
    "    for userID, g_df in tqdm(userID_df):\n",
    "        testId_df = g_df.groupby('testId')\n",
    "        for testId, gg_df in testId_df:\n",
    "            index_list = gg_df.index.tolist()\n",
    "            start_index = 0\n",
    "            if len(gg_df) <= testId2len[testId]:\n",
    "                start_index_list += [index_list[start_index]]\n",
    "                second_index_list += [index_list[start_index + 1]]\n",
    "            else:\n",
    "                div = len(gg_df) // testId2len[testId]\n",
    "                for _ in range(div):\n",
    "                    start_index_list += [index_list[start_index]]\n",
    "                    second_index_list += [index_list[start_index + 1]]\n",
    "                    start_index += testId2len[testId]\n",
    "\n",
    "    df.loc[start_index_list, 'diff_Timestamp'] = df.loc[second_index_list, 'diff_Timestamp'].values\n",
    "    df['elapsed'] = df['diff_Timestamp'].apply(lambda x: x.total_seconds() if not pd.isna(x) else np.nan)\n",
    "\n",
    "\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek # 요일을 숫자로\n",
    "\n",
    "    diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "    # 문제별 풀이시간\n",
    "    df['elapsed'] = diff\n",
    "    df['elapsed'] = df['elapsed'].apply(lambda x : x if x <650 and x >=0 else 0)\n",
    "\n",
    "    df['testcode']=df['testId'].apply(lambda x : int(x[1:4])//10)\n",
    "    df['problem_number'] = df['assessmentItemID'].apply(lambda x: int(x[7:])) \n",
    "\n",
    "\n",
    "    # feature 별 정답여부\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "    correct_a = df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_a.columns = [\"ass_mean\", 'ass_sum']\n",
    "    correct_p = df.groupby(['problem_number'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_p.columns = [\"prb_mean\", 'prb_sum']\n",
    "    correct_h = df.groupby(['hour'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_h.columns = [\"hour_mean\", 'hour_sum']\n",
    "    correct_d = df.groupby(['dow'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_d.columns = [\"dow_mean\", 'dow_sum'] \n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, correct_a, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, correct_p, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, correct_h, on=['hour'], how=\"left\")\n",
    "    df = pd.merge(df, correct_d, on=['dow'], how=\"left\")\n",
    "\n",
    "\n",
    "    # 정답과 오답 기준으로 나눠서 생각\n",
    "    o_df = df[df['answerCode']==1]\n",
    "    x_df = df[df['answerCode']==0]\n",
    "\n",
    "    elp_k = df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k.columns = ['KnowledgeTag',\"tag_elp\"]\n",
    "    elp_k_o = o_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_o.columns = ['KnowledgeTag', \"tag_elp_o\"]\n",
    "    elp_k_x = x_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_x.columns = ['KnowledgeTag', \"tag_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, elp_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_o, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_x, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "    ass_k = df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k.columns = ['assessmentItemID',\"ass_elp\"]\n",
    "    ass_k_o = o_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_o.columns = ['assessmentItemID',\"ass_elp_o\"]\n",
    "    ass_k_x = x_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_x.columns = ['assessmentItemID',\"ass_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, ass_k, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_o, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_x, on=['assessmentItemID'], how=\"left\")\n",
    "\n",
    "    prb_k = df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k.columns = ['problem_number',\"prb_elp\"]\n",
    "    prb_k_o = o_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_o.columns = ['problem_number',\"prb_elp_o\"]\n",
    "    prb_k_x = x_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_x.columns = ['problem_number',\"prb_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, prb_k, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_o, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_x, on=['problem_number'], how=\"left\")\n",
    "\n",
    "    # 누적합 - 주어진 데이터 이전/이후 데이터들을 포함하는 메모리를 feature로 포함시킴: Sequence Model을 사용하지 않고 일반적인 지도 학습 모델에서 사용하기 위함\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "    df['testcode_o'] = df.groupby(['userID','testcode'])['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeCount'] = df.groupby(['userID','testcode']).cumcount()\n",
    "    df['testcodeAcc'] = df['testcode_o']/df['testcodeCount']\n",
    "    df['tectcodeElp'] = df.groupby(['userID','testcode'])['elapsed'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeMElp'] = df['tectcodeElp']/df['testcodeCount']\n",
    "\n",
    "\n",
    "\n",
    "    f = lambda x : len(set(x))\n",
    "    t_df = df.groupby(['testId']).agg({\n",
    "    'problem_number':'max',\n",
    "    'KnowledgeTag':f\n",
    "    })\n",
    "    t_df.reset_index(inplace=True)\n",
    "\n",
    "    t_df.columns = ['testId','problem_count',\"tag_count\"]\n",
    "\n",
    "    df = pd.merge(df,t_df,on='testId',how='left')\n",
    "\n",
    "    gdf = df[['userID','testId','problem_number','testcode','Timestamp']].sort_values(by=['userID','testcode','Timestamp'])\n",
    "    gdf['buserID'] = gdf['userID'] != gdf['userID'].shift(1)\n",
    "    gdf['btestcode'] = gdf['testcode'] != gdf['testcode'].shift(1)\n",
    "    gdf['first'] = gdf[['buserID','btestcode']].any(axis=1).apply(lambda x : 1- int(x))\n",
    "    gdf['RepeatedTime'] = gdf['Timestamp'].diff().fillna(pd.Timedelta(seconds=0)) \n",
    "    gdf['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x: x.total_seconds()) * gdf['first']\n",
    "    df['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x : math.log(x+1))\n",
    "\n",
    "    df['prior_KnowledgeTag_frequency'] = df.groupby(['userID','KnowledgeTag']).cumcount()\n",
    "\n",
    "    df['problem_position'] = df['problem_number'] / df[\"problem_count\"]\n",
    "    df['solve_order'] = df.groupby(['userID','testId']).cumcount()\n",
    "    df['solve_order'] = df['solve_order'] - df['problem_count']*(df['solve_order'] > df['problem_count']).apply(int) + 1\n",
    "    df['retest'] = (df['solve_order'] > df['problem_count']).apply(int)\n",
    "    T = df['solve_order'] != df['problem_number']\n",
    "    TT = T.shift(1)\n",
    "    TT[0] = False\n",
    "    df['solved_disorder'] = (TT.apply(lambda x : not x) & T).apply(int)\n",
    "\n",
    "    df['testId'] = df['testId'].apply(lambda x : int(x[1:4]+x[-3]))\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek\n",
    "\n",
    "    return df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [00:33<00:00, 201.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#df = FeatureEngineering.FE(df)\n",
    "#df.to_csv(DATA_PATH + 'train_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH+'train_featured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "random.seed(42)\n",
    "def custom_train_test_split(df, ratio=0.8, split=True):\n",
    "    \n",
    "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    max_train_data_len = ratio*len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids =[]\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if max_train_data_len < sum_of_train_data:\n",
    "            break\n",
    "        user_ids.append(user_id)\n",
    "\n",
    "\n",
    "    train = df[df['userID'].isin(user_ids)]\n",
    "    test = df[df['userID'].isin(user_ids) == False]\n",
    "\n",
    "    #test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "    test = test[test['userID'] != test['userID'].shift(-1)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별 분리\n",
    "train, test = custom_train_test_split(df)\n",
    "\n",
    "# 사용할 Feature 설정\n",
    "FEATS = df.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "# X, y 값 분리\n",
    "y_train = train['answerCode']\n",
    "train = train.drop(['answerCode'], axis=1)\n",
    "\n",
    "y_test = test['answerCode']\n",
    "test = test.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train[FEATS], y_train)\n",
    "lgb_test = lgb.Dataset(test[FEATS], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1187785, number of negative: 624671\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6979\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812456, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655346 -> initscore=0.642620\n",
      "[LightGBM] [Info] Start training from score 0.642620\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.429549\tvalid_1's binary_logloss: 0.467797\n",
      "[200]\ttraining's binary_logloss: 0.425569\tvalid_1's binary_logloss: 0.461142\n",
      "[300]\ttraining's binary_logloss: 0.422893\tvalid_1's binary_logloss: 0.457344\n",
      "[400]\ttraining's binary_logloss: 0.420597\tvalid_1's binary_logloss: 0.453137\n",
      "[500]\ttraining's binary_logloss: 0.418485\tvalid_1's binary_logloss: 0.451562\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.418485\tvalid_1's binary_logloss: 0.451562\n",
      "VALID AUC : 0.8674416564309527 ACC : 0.7779422649888971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    {'objective': 'binary'}, \n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_test],\n",
    "    verbose_eval=100,\n",
    "    num_boost_round=500,\n",
    "    early_stopping_rounds=100\n",
    ")\n",
    "\n",
    "preds = model.predict(test[FEATS])\n",
    "acc = accuracy_score(y_test, np.where(preds >= 0.5, 1, 0))\n",
    "auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(f'VALID AUC : {auc} ACC : {acc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:03<00:00, 197.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# FEATURE ENGINEERING\n",
    "#test_df = FeatureEngineering.FE(test_df)\n",
    "#test_df.to_csv(DATA_PATH + 'test_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "test_df = pd.read_csv(DATA_PATH+'test_featured.csv')\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PREDICTION\n",
    "total_preds = model.predict(test_df[FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(DATA_PATH+'sample_submission.csv')\n",
    "submission['prediction'] = total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(DATA_PATH+'lgbm_base_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdfeffe # 현정이가 train-test 다르게 처리 한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for the LightGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # For regression tasks\n",
    "    'metric': 'rmse',  # Root Mean Squared Error as the evaluation metric\n",
    "    'num_leaves': 31,  # Maximum number of leaves in one tree\n",
    "    'learning_rate': 0.05,  # Learning rate for boosting\n",
    "    'feature_fraction': 0.9,  # Fraction of features to be used per tree\n",
    "    'bagging_fraction': 0.8,  # Fraction of data to be bagged\n",
    "    'bagging_freq': 5,  # Frequency of bagging\n",
    "    'verbose': 0  # Verbosity of output\n",
    "}\n",
    "\n",
    "# Initialize a list to store the cross-validation scores\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([      0,       2,       3,       4,       5,       6,       7,\\n                  8,       9,      10,\\n            ...\\n            2266574, 2266575, 2266576, 2266578, 2266579, 2266581, 2266582,\\n            2266583, 2266584, 2266585],\\n           dtype='int64', length=1813268)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m kf \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(X):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Split the data into training and testing sets for this fold\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m X[train_index], X[test_index]\n\u001b[1;32m      6\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m y[train_index], y[test_index]\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Create the LightGBM dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([      0,       2,       3,       4,       5,       6,       7,\\n                  8,       9,      10,\\n            ...\\n            2266574, 2266575, 2266576, 2266578, 2266579, 2266581, 2266582,\\n            2266583, 2266584, 2266585],\\n           dtype='int64', length=1813268)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Perform K-fold cross-validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_binary = np.round(y_pred)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Compute AUROC\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auroc_scores.append(auroc)\n",
    "\n",
    "    # Print the evaluation metrics for this fold\n",
    "    print('Fold Accuracy:', accuracy)\n",
    "    print('Fold AUROC:', auroc)\n",
    "    print('---')\n",
    "\n",
    "# Calculate the mean and standard deviation of the evaluation metrics\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "mean_auroc = np.mean(auroc_scores)\n",
    "std_auroc = np.std(auroc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
