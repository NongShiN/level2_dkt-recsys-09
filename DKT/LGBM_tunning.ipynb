{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "import missingno as msno\n",
    "import os\n",
    "#from data_loader import FeatureEngineering\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.14 s, sys: 376 ms, total: 4.52 s\n",
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtype = {\n",
    "    'userID': 'int16',\n",
    "    'answerCode': 'int8',\n",
    "    'KnowledgeTag': 'int16'\n",
    "}   \n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "df = df.sort_values(by=['userID', 'Timestamp', 'testId']).reset_index(drop=True)\n",
    "copy_df = df.copy()\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def feature_engineering(df):\n",
    "    # 문제별 풀이시간\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['diff_Timestamp'] = df['Timestamp'] - df.shift(1)['Timestamp']\n",
    "\n",
    "    testId_df = df[~df.duplicated(['assessmentItemID'])].groupby('testId')\n",
    "    testId2len = {}\n",
    "    for testId, g_df in testId_df:\n",
    "        testId2len[testId] = len(g_df)\n",
    "\n",
    "    userID_df = df.groupby('userID')\n",
    "    start_index_list = []\n",
    "    second_index_list = []\n",
    "\n",
    "    for userID, g_df in tqdm(userID_df):\n",
    "        testId_df = g_df.groupby('testId')\n",
    "        for testId, gg_df in testId_df:\n",
    "            index_list = gg_df.index.tolist()\n",
    "            start_index = 0\n",
    "            if len(gg_df) <= testId2len[testId]:\n",
    "                start_index_list += [index_list[start_index]]\n",
    "                second_index_list += [index_list[start_index + 1]]\n",
    "            else:\n",
    "                div = len(gg_df) // testId2len[testId]\n",
    "                for _ in range(div):\n",
    "                    start_index_list += [index_list[start_index]]\n",
    "                    second_index_list += [index_list[start_index + 1]]\n",
    "                    start_index += testId2len[testId]\n",
    "\n",
    "    df.loc[start_index_list, 'diff_Timestamp'] = df.loc[second_index_list, 'diff_Timestamp'].values\n",
    "    df['elapsed'] = df['diff_Timestamp'].apply(lambda x: x.total_seconds() if not pd.isna(x) else np.nan)\n",
    "\n",
    "\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek # 요일을 숫자로\n",
    "\n",
    "    diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "    # 문제별 풀이시간\n",
    "    df['elapsed'] = diff\n",
    "    df['elapsed'] = df['elapsed'].apply(lambda x : x if x <650 and x >=0 else 0)\n",
    "\n",
    "    df['testcode']=df['testId'].apply(lambda x : int(x[1:4])//10)\n",
    "    df['problem_number'] = df['assessmentItemID'].apply(lambda x: int(x[7:])) \n",
    "\n",
    "\n",
    "    # feature 별 정답여부\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "    correct_a = df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_a.columns = [\"ass_mean\", 'ass_sum']\n",
    "    correct_p = df.groupby(['problem_number'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_p.columns = [\"prb_mean\", 'prb_sum']\n",
    "    correct_h = df.groupby(['hour'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_h.columns = [\"hour_mean\", 'hour_sum']\n",
    "    correct_d = df.groupby(['dow'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_d.columns = [\"dow_mean\", 'dow_sum'] \n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, correct_a, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, correct_p, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, correct_h, on=['hour'], how=\"left\")\n",
    "    df = pd.merge(df, correct_d, on=['dow'], how=\"left\")\n",
    "\n",
    "\n",
    "    # 정답과 오답 기준으로 나눠서 생각\n",
    "    o_df = df[df['answerCode']==1]\n",
    "    x_df = df[df['answerCode']==0]\n",
    "\n",
    "    elp_k = df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k.columns = ['KnowledgeTag',\"tag_elp\"]\n",
    "    elp_k_o = o_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_o.columns = ['KnowledgeTag', \"tag_elp_o\"]\n",
    "    elp_k_x = x_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_x.columns = ['KnowledgeTag', \"tag_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, elp_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_o, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_x, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "    ass_k = df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k.columns = ['assessmentItemID',\"ass_elp\"]\n",
    "    ass_k_o = o_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_o.columns = ['assessmentItemID',\"ass_elp_o\"]\n",
    "    ass_k_x = x_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_x.columns = ['assessmentItemID',\"ass_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, ass_k, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_o, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_x, on=['assessmentItemID'], how=\"left\")\n",
    "\n",
    "    prb_k = df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k.columns = ['problem_number',\"prb_elp\"]\n",
    "    prb_k_o = o_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_o.columns = ['problem_number',\"prb_elp_o\"]\n",
    "    prb_k_x = x_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_x.columns = ['problem_number',\"prb_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, prb_k, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_o, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_x, on=['problem_number'], how=\"left\")\n",
    "\n",
    "    # 누적합 - 주어진 데이터 이전/이후 데이터들을 포함하는 메모리를 feature로 포함시킴: Sequence Model을 사용하지 않고 일반적인 지도 학습 모델에서 사용하기 위함\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "    df['testcode_o'] = df.groupby(['userID','testcode'])['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeCount'] = df.groupby(['userID','testcode']).cumcount()\n",
    "    df['testcodeAcc'] = df['testcode_o']/df['testcodeCount']\n",
    "    df['tectcodeElp'] = df.groupby(['userID','testcode'])['elapsed'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeMElp'] = df['tectcodeElp']/df['testcodeCount']\n",
    "\n",
    "\n",
    "\n",
    "    f = lambda x : len(set(x))\n",
    "    t_df = df.groupby(['testId']).agg({\n",
    "    'problem_number':'max',\n",
    "    'KnowledgeTag':f\n",
    "    })\n",
    "    t_df.reset_index(inplace=True)\n",
    "\n",
    "    t_df.columns = ['testId','problem_count',\"tag_count\"]\n",
    "\n",
    "    df = pd.merge(df,t_df,on='testId',how='left')\n",
    "\n",
    "    gdf = df[['userID','testId','problem_number','testcode','Timestamp']].sort_values(by=['userID','testcode','Timestamp'])\n",
    "    gdf['buserID'] = gdf['userID'] != gdf['userID'].shift(1)\n",
    "    gdf['btestcode'] = gdf['testcode'] != gdf['testcode'].shift(1)\n",
    "    gdf['first'] = gdf[['buserID','btestcode']].any(axis=1).apply(lambda x : 1- int(x))\n",
    "    gdf['RepeatedTime'] = gdf['Timestamp'].diff().fillna(pd.Timedelta(seconds=0)) \n",
    "    gdf['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x: x.total_seconds()) * gdf['first']\n",
    "    df['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x : math.log(x+1))\n",
    "\n",
    "    df['prior_KnowledgeTag_frequency'] = df.groupby(['userID','KnowledgeTag']).cumcount()\n",
    "\n",
    "    df['problem_position'] = df['problem_number'] / df[\"problem_count\"]\n",
    "    df['solve_order'] = df.groupby(['userID','testId']).cumcount()\n",
    "    df['solve_order'] = df['solve_order'] - df['problem_count']*(df['solve_order'] > df['problem_count']).apply(int) + 1\n",
    "    df['retest'] = (df['solve_order'] > df['problem_count']).apply(int)\n",
    "    T = df['solve_order'] != df['problem_number']\n",
    "    TT = T.shift(1)\n",
    "    TT[0] = False\n",
    "    df['solved_disorder'] = (TT.apply(lambda x : not x) & T).apply(int)\n",
    "\n",
    "    df['testId'] = df['testId'].apply(lambda x : int(x[1:4]+x[-3]))\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek\n",
    "\n",
    "    return df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [00:33<00:00, 201.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#df = FeatureEngineering.FE(df)\n",
    "#df.to_csv(DATA_PATH + 'train_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH+'train_featured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhcw1027\u001b[0m (\u001b[33mffm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/input/level2_dkt-recsys-09/DKT/wandb/run-20230516_040934-1kad5hly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ffm/LGBM_tunning_1/runs/1kad5hly' target=\"_blank\">crimson-silence-2</a></strong> to <a href='https://wandb.ai/ffm/LGBM_tunning_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ffm/LGBM_tunning_1' target=\"_blank\">https://wandb.ai/ffm/LGBM_tunning_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ffm/LGBM_tunning_1/runs/1kad5hly' target=\"_blank\">https://wandb.ai/ffm/LGBM_tunning_1/runs/1kad5hly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up Wandb\n",
    "wandb.init(project=\"LGBM_tunning_1\", entity=\"ffm\")\n",
    "\n",
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "random.seed(42)\n",
    "def custom_train_test_split(df, ratio=0.8, split=True):\n",
    "    \n",
    "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    max_train_data_len = ratio * len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids =[]\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if max_train_data_len < sum_of_train_data:\n",
    "            break\n",
    "        user_ids.append(user_id)\n",
    "\n",
    "\n",
    "    train = df[df['userID'].isin(user_ids)]\n",
    "    test = df[df['userID'].isin(user_ids) == False]\n",
    "\n",
    "    # test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "    test = test[test['userID'] != test['userID'].shift(-1)]\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별 분리\n",
    "train, test = custom_train_test_split(df)\n",
    "\n",
    "# 사용할 Feature 설정\n",
    "FEATS = df.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "# X, y 값 분리\n",
    "y_train = train['answerCode']\n",
    "train = train.drop(['answerCode'], axis=1)\n",
    "\n",
    "y_test = test['answerCode']\n",
    "test = test.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train[FEATS], y_train)\n",
    "lgb_test = lgb.Dataset(test[FEATS], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna optimization\n",
    "def objective(trial: Trial, lgb_train, lgb_test):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    def log_metrics(env):\n",
    "        eval_results = env.evaluation_result_list\n",
    "        if len(eval_results) > 0:\n",
    "            eval_result = eval_results[0]  # Assuming only one tuple is returned\n",
    "            metric, value = eval_result\n",
    "            wandb.log({metric: value})\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[log_metrics]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(test[FEATS])\n",
    "    acc = accuracy_score(y_test, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    wandb.log({\"ACC\": acc, \"AUC\": auc})\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-16 04:22:14,139]\u001b[0m A new study created in memory with name: no-name-1d0d36b7-00dd-4591-86db-24afcbdf504c\u001b[0m\n",
      "/tmp/ipykernel_2873/247369343.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
      "/tmp/ipykernel_2873/247369343.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
      "/tmp/ipykernel_2873/247369343.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
      "/tmp/ipykernel_2873/247369343.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
      "/tmp/ipykernel_2873/247369343.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6977\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-16 04:22:14,693]\u001b[0m Trial 0 failed with parameters: {'num_leaves': 301, 'learning_rate': 0.014321465021202125, 'feature_fraction': 0.948040326440955, 'bagging_fraction': 0.35927524134703515, 'bagging_freq': 10, 'lambda_l1': 8.817241244286954e-05, 'lambda_l2': 0.01903839712942754} because of the following error: ValueError('too many values to unpack (expected 2)').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2873/847938033.py\", line 3, in <lambda>\n",
      "    study.optimize(lambda trial : objective(trial, lgb_train, lgb_test), n_trials=100)\n",
      "  File \"/tmp/ipykernel_2873/247369343.py\", line 24, in objective\n",
      "    model = lgb.train(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py\", line 302, in train\n",
      "    cb(callback.CallbackEnv(model=booster,\n",
      "  File \"/tmp/ipykernel_2873/247369343.py\", line 21, in log_metrics\n",
      "    metric, value = eval_result\n",
      "ValueError: too many values to unpack (expected 2)\n",
      "\u001b[33m[W 2023-05-16 04:22:14,695]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Optimize hyperparameters with Optuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler())\n\u001b[0;32m----> 3\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial : objective(trial, lgb_train, lgb_test), n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Optimize hyperparameters with Optuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler())\n\u001b[0;32m----> 3\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial : objective(trial, lgb_train, lgb_test), n_trials\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, lgb_train, lgb_test)\u001b[0m\n\u001b[1;32m     21\u001b[0m         metric, value \u001b[39m=\u001b[39m eval_result\n\u001b[1;32m     22\u001b[0m         wandb\u001b[39m.\u001b[39mlog({metric: value})\n\u001b[0;32m---> 24\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     25\u001b[0m     params, \n\u001b[1;32m     26\u001b[0m     lgb_train,\n\u001b[1;32m     27\u001b[0m     valid_sets\u001b[39m=\u001b[39;49m[lgb_train, lgb_test],\n\u001b[1;32m     28\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[log_metrics]\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test[FEATS])\n\u001b[1;32m     35\u001b[0m acc \u001b[39m=\u001b[39m accuracy_score(y_test, np\u001b[39m.\u001b[39mwhere(preds \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:302\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_after_iter:\n\u001b[0;32m--> 302\u001b[0m         cb(callback\u001b[39m.\u001b[39;49mCallbackEnv(model\u001b[39m=\u001b[39;49mbooster,\n\u001b[1;32m    303\u001b[0m                                 params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    304\u001b[0m                                 iteration\u001b[39m=\u001b[39;49mi,\n\u001b[1;32m    305\u001b[0m                                 begin_iteration\u001b[39m=\u001b[39;49minit_iteration,\n\u001b[1;32m    306\u001b[0m                                 end_iteration\u001b[39m=\u001b[39;49minit_iteration \u001b[39m+\u001b[39;49m num_boost_round,\n\u001b[1;32m    307\u001b[0m                                 evaluation_result_list\u001b[39m=\u001b[39;49mevaluation_result_list))\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m callback\u001b[39m.\u001b[39mEarlyStopException \u001b[39mas\u001b[39;00m earlyStopException:\n\u001b[1;32m    309\u001b[0m     booster\u001b[39m.\u001b[39mbest_iteration \u001b[39m=\u001b[39m earlyStopException\u001b[39m.\u001b[39mbest_iteration \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m, in \u001b[0;36mobjective.<locals>.log_metrics\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(eval_results) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     eval_result \u001b[39m=\u001b[39m eval_results[\u001b[39m0\u001b[39m]  \u001b[39m# Assuming only one tuple is returned\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     metric, value \u001b[39m=\u001b[39m eval_result\n\u001b[1;32m     22\u001b[0m     wandb\u001b[39m.\u001b[39mlog({metric: value})\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Optimize hyperparameters with Optuna\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler())\n",
    "study.optimize(lambda trial : objective(trial, lgb_train, lgb_test), n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get the best parameters and retrain the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39;49mbest_trial\u001b[39m.\u001b[39mparams\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m      5\u001b[0m     best_params, \n\u001b[1;32m      6\u001b[0m     lgb_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     callbacks\u001b[39m=\u001b[39m[wandb_callback]\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/study.py:159\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49mget_best_trial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_study_id))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/storages/_in_memory.py:250\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    247\u001b[0m best_trial_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mbest_trial_id\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m best_trial_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo trials are completed yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mdirections) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "# Get the best parameters and retrain the model\n",
    "if study.best_trial is not None:\n",
    "    best_params = study.best_trial.params\n",
    "    model = lgb.train(\n",
    "        best_params, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[wandb_callback]\n",
    "    )\n",
    "else:\n",
    "    print(\"No trials were completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:03<00:00, 197.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# FEATURE ENGINEERING\n",
    "#test_df = FeatureEngineering.FE(test_df)\n",
    "#test_df.to_csv(DATA_PATH + 'test_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "test_df = pd.read_csv(DATA_PATH+'test_featured.csv')\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PREDICTION\n",
    "total_preds = model.predict(test_df[FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(DATA_PATH+'sample_submission.csv')\n",
    "submission['prediction'] = total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(DATA_PATH+'lgbm_base_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdfeffe # 현정이가 train-test 다르게 처리 한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for the LightGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # For regression tasks\n",
    "    'metric': 'rmse',  # Root Mean Squared Error as the evaluation metric\n",
    "    'num_leaves': 31,  # Maximum number of leaves in one tree\n",
    "    'learning_rate': 0.05,  # Learning rate for boosting\n",
    "    'feature_fraction': 0.9,  # Fraction of features to be used per tree\n",
    "    'bagging_fraction': 0.8,  # Fraction of data to be bagged\n",
    "    'bagging_freq': 5,  # Frequency of bagging\n",
    "    'verbose': 0  # Verbosity of output\n",
    "}\n",
    "\n",
    "# Initialize a list to store the cross-validation scores\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([      0,       2,       3,       4,       5,       6,       7,\\n                  8,       9,      10,\\n            ...\\n            2266574, 2266575, 2266576, 2266578, 2266579, 2266581, 2266582,\\n            2266583, 2266584, 2266585],\\n           dtype='int64', length=1813268)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m kf \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(X):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Split the data into training and testing sets for this fold\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m X[train_index], X[test_index]\n\u001b[1;32m      6\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m y[train_index], y[test_index]\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Create the LightGBM dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([      0,       2,       3,       4,       5,       6,       7,\\n                  8,       9,      10,\\n            ...\\n            2266574, 2266575, 2266576, 2266578, 2266579, 2266581, 2266582,\\n            2266583, 2266584, 2266585],\\n           dtype='int64', length=1813268)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Perform K-fold cross-validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_binary = np.round(y_pred)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Compute AUROC\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    auroc_scores.append(auroc)\n",
    "\n",
    "    # Print the evaluation metrics for this fold\n",
    "    print('Fold Accuracy:', accuracy)\n",
    "    print('Fold AUROC:', auroc)\n",
    "    print('---')\n",
    "\n",
    "# Calculate the mean and standard deviation of the evaluation metrics\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "mean_auroc = np.mean(auroc_scores)\n",
    "std_auroc = np.std(auroc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
