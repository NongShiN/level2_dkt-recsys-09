{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09d4aa4-327c-4149-b592-68ac81c6540b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "import os\n",
    "from math import pi\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "import json\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61dfd890-482a-41de-8faf-4f4d9dd3ff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    # 문제별 풀이시간\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['diff_Timestamp'] = df['Timestamp'] - df.shift(1)['Timestamp']\n",
    "\n",
    "    testId_df = df[~df.duplicated(['assessmentItemID'])].groupby('testId')\n",
    "    testId2len = {}\n",
    "    for testId, g_df in testId_df:\n",
    "        testId2len[testId] = len(g_df)\n",
    "\n",
    "    userID_df = df.groupby('userID')\n",
    "    start_index_list = []\n",
    "    second_index_list = []\n",
    "\n",
    "    for userID, g_df in tqdm(userID_df):\n",
    "        testId_df = g_df.groupby('testId')\n",
    "        for testId, gg_df in testId_df:\n",
    "            index_list = gg_df.index.tolist()\n",
    "            start_index = 0\n",
    "            if len(gg_df) <= testId2len[testId]:\n",
    "                start_index_list += [index_list[start_index]]\n",
    "                second_index_list += [index_list[start_index + 1]]\n",
    "            else:\n",
    "                div = len(gg_df) // testId2len[testId]\n",
    "                for _ in range(div):\n",
    "                    start_index_list += [index_list[start_index]]\n",
    "                    second_index_list += [index_list[start_index + 1]]\n",
    "                    start_index += testId2len[testId]\n",
    "\n",
    "    df.loc[start_index_list, 'diff_Timestamp'] = df.loc[second_index_list, 'diff_Timestamp'].values\n",
    "    df['elapsed'] = df['diff_Timestamp'].apply(lambda x: x.total_seconds() if not pd.isna(x) else np.nan)\n",
    "\n",
    "\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek # 요일을 숫자로\n",
    "\n",
    "    diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "    # 문제별 풀이시간\n",
    "    df['elapsed'] = diff\n",
    "    df['elapsed'] = df['elapsed'].apply(lambda x : x if x <650 and x >=0 else 0)\n",
    "\n",
    "    df['testcode']=df['testId'].apply(lambda x : int(x[1:4])//10)\n",
    "    df['problem_number'] = df['assessmentItemID'].apply(lambda x: int(x[7:])) \n",
    "\n",
    "\n",
    "    # feature 별 정답여부\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "    correct_a = df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_a.columns = [\"ass_mean\", 'ass_sum']\n",
    "    correct_p = df.groupby(['problem_number'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_p.columns = [\"prb_mean\", 'prb_sum']\n",
    "    correct_h = df.groupby(['hour'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_h.columns = [\"hour_mean\", 'hour_sum']\n",
    "    correct_d = df.groupby(['dow'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_d.columns = [\"dow_mean\", 'dow_sum'] \n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, correct_a, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, correct_p, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, correct_h, on=['hour'], how=\"left\")\n",
    "    df = pd.merge(df, correct_d, on=['dow'], how=\"left\")\n",
    "\n",
    "\n",
    "    # 정답과 오답 기준으로 나눠서 생각\n",
    "    o_df = df[df['answerCode']==1]\n",
    "    x_df = df[df['answerCode']==0]\n",
    "\n",
    "    elp_k = df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k.columns = ['KnowledgeTag',\"tag_elp\"]\n",
    "    elp_k_o = o_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_o.columns = ['KnowledgeTag', \"tag_elp_o\"]\n",
    "    elp_k_x = x_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_x.columns = ['KnowledgeTag', \"tag_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, elp_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_o, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_x, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "    ass_k = df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k.columns = ['assessmentItemID',\"ass_elp\"]\n",
    "    ass_k_o = o_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_o.columns = ['assessmentItemID',\"ass_elp_o\"]\n",
    "    ass_k_x = x_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_x.columns = ['assessmentItemID',\"ass_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, ass_k, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_o, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_x, on=['assessmentItemID'], how=\"left\")\n",
    "\n",
    "    prb_k = df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k.columns = ['problem_number',\"prb_elp\"]\n",
    "    prb_k_o = o_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_o.columns = ['problem_number',\"prb_elp_o\"]\n",
    "    prb_k_x = x_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_x.columns = ['problem_number',\"prb_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, prb_k, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_o, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_x, on=['problem_number'], how=\"left\")\n",
    "\n",
    "    # 누적합 - 주어진 데이터 이전/이후 데이터들을 포함하는 메모리를 feature로 포함시킴: Sequence Model을 사용하지 않고 일반적인 지도 학습 모델에서 사용하기 위함\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "    df['testcode_o'] = df.groupby(['userID','testcode'])['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeCount'] = df.groupby(['userID','testcode']).cumcount()\n",
    "    df['testcodeAcc'] = df['testcode_o']/df['testcodeCount']\n",
    "    df['tectcodeElp'] = df.groupby(['userID','testcode'])['elapsed'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeMElp'] = df['tectcodeElp']/df['testcodeCount']\n",
    "\n",
    "\n",
    "\n",
    "    f = lambda x : len(set(x))\n",
    "    t_df = df.groupby(['testId']).agg({\n",
    "    'problem_number':'max',\n",
    "    'KnowledgeTag':f\n",
    "    })\n",
    "    t_df.reset_index(inplace=True)\n",
    "\n",
    "    t_df.columns = ['testId','problem_count',\"tag_count\"]\n",
    "\n",
    "    df = pd.merge(df,t_df,on='testId',how='left')\n",
    "\n",
    "    gdf = df[['userID','testId','problem_number','testcode','Timestamp']].sort_values(by=['userID','testcode','Timestamp'])\n",
    "    gdf['buserID'] = gdf['userID'] != gdf['userID'].shift(1)\n",
    "    gdf['btestcode'] = gdf['testcode'] != gdf['testcode'].shift(1)\n",
    "    gdf['first'] = gdf[['buserID','btestcode']].any(axis=1).apply(lambda x : 1- int(x))\n",
    "    gdf['RepeatedTime'] = gdf['Timestamp'].diff().fillna(pd.Timedelta(seconds=0)) \n",
    "    gdf['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x: x.total_seconds()) * gdf['first']\n",
    "    df['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x : math.log(x+1))\n",
    "\n",
    "    df['prior_KnowledgeTag_frequency'] = df.groupby(['userID','KnowledgeTag']).cumcount()\n",
    "\n",
    "    df['problem_position'] = df['problem_number'] / df[\"problem_count\"]\n",
    "    df['solve_order'] = df.groupby(['userID','testId']).cumcount()\n",
    "    df['solve_order'] = df['solve_order'] - df['problem_count']*(df['solve_order'] > df['problem_count']).apply(int) + 1\n",
    "    df['retest'] = (df['solve_order'] > df['problem_count']).apply(int)\n",
    "    T = df['solve_order'] != df['problem_number']\n",
    "    TT = T.shift(1)\n",
    "    TT[0] = False\n",
    "    df['solved_disorder'] = (TT.apply(lambda x : not x) & T).apply(int)\n",
    "\n",
    "    df['testId'] = df['testId'].apply(lambda x : int(x[1:4]+x[-3]))\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dadee14f-b7cc-4e98-9014-682eba8f39f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "os.chdir('/opt/ml/level2_dkt-recsys-09/DKT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc16ae4-fc7b-481b-89f7-11125d7dd210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [00:20<00:00, 326.75it/s] \n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/opt/ml/input/data'\n",
    "\n",
    "%time\n",
    "dtype = {\n",
    "    'userID': 'int16',\n",
    "    'answerCode': 'int8',\n",
    "    'KnowledgeTag': 'int16'\n",
    "}   \n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "df = df.sort_values(by=['userID', 'Timestamp', 'testId']).reset_index(drop=True)\n",
    "\n",
    "df = feature_engineering(df)\n",
    "df.to_csv(DATA_PATH + 'train_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a62e61b-b53c-44ca-a4b7-183c4e73ee79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------START FOLD 1 TRAINING---------------------------\n",
      "-------------------------START FOLD 1 MODEL LOADING----------------------\n",
      "(1806456, 48) (441630, 48)\n",
      "[LightGBM] [Info] Number of positive: 1181418, number of negative: 625038\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6470\n",
      "[LightGBM] [Info] Number of data points in the train set: 1806456, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653998 -> initscore=0.636658\n",
      "[LightGBM] [Info] Start training from score 0.636658\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.446834\tvalid_1's binary_logloss: 0.450146\n",
      "[200]\ttraining's binary_logloss: 0.442826\tvalid_1's binary_logloss: 0.448247\n",
      "[300]\ttraining's binary_logloss: 0.44008\tvalid_1's binary_logloss: 0.447588\n",
      "[400]\ttraining's binary_logloss: 0.437808\tvalid_1's binary_logloss: 0.447161\n",
      "[500]\ttraining's binary_logloss: 0.435706\tvalid_1's binary_logloss: 0.446944\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.435706\tvalid_1's binary_logloss: 0.446944\n",
      "-------------------------DONE FOLD 1 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8492506490596643 ACC : 0.7947897561306977\n",
      "\n",
      "---------------------------DONE FOLD 1 TRAINING--------------------------\n",
      "-------------------------START FOLD 2 TRAINING---------------------------\n",
      "-------------------------START FOLD 2 MODEL LOADING----------------------\n",
      "(1804088, 48) (443998, 48)\n",
      "[LightGBM] [Info] Number of positive: 1184070, number of negative: 620018\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1804088, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656326 -> initscore=0.646964\n",
      "[LightGBM] [Info] Start training from score 0.646964\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447169\tvalid_1's binary_logloss: 0.449072\n",
      "[200]\ttraining's binary_logloss: 0.443433\tvalid_1's binary_logloss: 0.447529\n",
      "[300]\ttraining's binary_logloss: 0.440797\tvalid_1's binary_logloss: 0.44675\n",
      "[400]\ttraining's binary_logloss: 0.438366\tvalid_1's binary_logloss: 0.446181\n",
      "[500]\ttraining's binary_logloss: 0.436214\tvalid_1's binary_logloss: 0.445973\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436214\tvalid_1's binary_logloss: 0.445973\n",
      "-------------------------DONE FOLD 2 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.852706889694067 ACC : 0.7948031297438277\n",
      "\n",
      "---------------------------DONE FOLD 2 TRAINING--------------------------\n",
      "-------------------------START FOLD 3 TRAINING---------------------------\n",
      "-------------------------START FOLD 3 MODEL LOADING----------------------\n",
      "(1799078, 48) (449008, 48)\n",
      "[LightGBM] [Info] Number of positive: 1181786, number of negative: 617292\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1799078, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656884 -> initscore=0.649440\n",
      "[LightGBM] [Info] Start training from score 0.649440\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.446282\tvalid_1's binary_logloss: 0.453144\n",
      "[200]\ttraining's binary_logloss: 0.442486\tvalid_1's binary_logloss: 0.451278\n",
      "[300]\ttraining's binary_logloss: 0.439729\tvalid_1's binary_logloss: 0.450553\n",
      "[400]\ttraining's binary_logloss: 0.437466\tvalid_1's binary_logloss: 0.450182\n",
      "[500]\ttraining's binary_logloss: 0.435364\tvalid_1's binary_logloss: 0.449902\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.435364\tvalid_1's binary_logloss: 0.449902\n",
      "-------------------------DONE FOLD 3 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8504228251046829 ACC : 0.792865605958023\n",
      "\n",
      "---------------------------DONE FOLD 3 TRAINING--------------------------\n",
      "-------------------------START FOLD 4 TRAINING---------------------------\n",
      "-------------------------START FOLD 4 MODEL LOADING----------------------\n",
      "(1782265, 48) (465821, 48)\n",
      "[LightGBM] [Info] Number of positive: 1167683, number of negative: 614582\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6469\n",
      "[LightGBM] [Info] Number of data points in the train set: 1782265, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655168 -> initscore=0.641834\n",
      "[LightGBM] [Info] Start training from score 0.641834\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447385\tvalid_1's binary_logloss: 0.448851\n",
      "[200]\ttraining's binary_logloss: 0.443332\tvalid_1's binary_logloss: 0.44702\n",
      "[300]\ttraining's binary_logloss: 0.440696\tvalid_1's binary_logloss: 0.446494\n",
      "[400]\ttraining's binary_logloss: 0.438281\tvalid_1's binary_logloss: 0.445959\n",
      "[500]\ttraining's binary_logloss: 0.436131\tvalid_1's binary_logloss: 0.445826\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436131\tvalid_1's binary_logloss: 0.445826\n",
      "-------------------------DONE FOLD 4 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8513649442001504 ACC : 0.7953999497661118\n",
      "\n",
      "---------------------------DONE FOLD 4 TRAINING--------------------------\n",
      "-------------------------START FOLD 5 TRAINING---------------------------\n",
      "-------------------------START FOLD 5 MODEL LOADING----------------------\n",
      "(1800457, 48) (447629, 48)\n",
      "[LightGBM] [Info] Number of positive: 1174663, number of negative: 625794\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6462\n",
      "[LightGBM] [Info] Number of data points in the train set: 1800457, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652425 -> initscore=0.629715\n",
      "[LightGBM] [Info] Start training from score 0.629715\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.448237\tvalid_1's binary_logloss: 0.445221\n",
      "[200]\ttraining's binary_logloss: 0.444305\tvalid_1's binary_logloss: 0.443485\n",
      "[300]\ttraining's binary_logloss: 0.441635\tvalid_1's binary_logloss: 0.442884\n",
      "[400]\ttraining's binary_logloss: 0.43924\tvalid_1's binary_logloss: 0.442502\n",
      "[500]\ttraining's binary_logloss: 0.437114\tvalid_1's binary_logloss: 0.442321\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.437114\tvalid_1's binary_logloss: 0.442321\n",
      "-------------------------DONE FOLD 5 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.849856271220662 ACC : 0.7973969514933126\n",
      "\n",
      "---------------------------DONE FOLD 5 TRAINING--------------------------\n"
     ]
    }
   ],
   "source": [
    "# userID index 기준 K-fold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train = df.copy()\n",
    "predicts_list = list()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    kf.split(train[\"userID\"].unique().tolist())\n",
    "):\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} TRAINING---------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} MODEL LOADING----------------------\"\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    \n",
    "    FEATS = train.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "    FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "    train = df.copy()\n",
    "    x_train = train[train['userID'].isin(train_idx)]\n",
    "    x_valid = train[train['userID'].isin(val_idx)]\n",
    "    X_train, Y_train = x_train.drop(['answerCode'], axis=1), x_train['answerCode']\n",
    "    X_valid, Y_valid = x_valid.drop(['answerCode'], axis=1), x_valid['answerCode']\n",
    "    # print(X_train.shape, X_valid.shape)\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train[FEATS], Y_train)\n",
    "    lgb_test = lgb.Dataset(X_valid[FEATS], Y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary'}, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"-------------------------DONE FOLD {fold + 1} MODEL LOADING-----------------------\"\n",
    "    )\n",
    "    predicts_list.append(model.predict(test_df[FEATS]))\n",
    "\n",
    "    preds = model.predict(X_valid[FEATS])\n",
    "    acc = accuracy_score(Y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(Y_valid, preds)\n",
    "\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    print(\n",
    "        f\"---------------------------DONE FOLD {fold + 1} TRAINING--------------------------\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4b7159c-22f7-43de-963f-3126976e4306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------START FOLD 1 TRAINING---------------------------\n",
      "-------------------------START FOLD 1 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186404, number of negative: 626864\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813268, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654290 -> initscore=0.637953\n",
      "[LightGBM] [Info] Start training from score 0.637953\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447301\tvalid_1's binary_logloss: 0.449423\n",
      "[200]\ttraining's binary_logloss: 0.443451\tvalid_1's binary_logloss: 0.44703\n",
      "[300]\ttraining's binary_logloss: 0.440961\tvalid_1's binary_logloss: 0.446065\n",
      "[400]\ttraining's binary_logloss: 0.438722\tvalid_1's binary_logloss: 0.445244\n",
      "[500]\ttraining's binary_logloss: 0.436591\tvalid_1's binary_logloss: 0.444539\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436591\tvalid_1's binary_logloss: 0.444539\n",
      "-------------------------DONE FOLD 1 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8521355356441817 ACC : 0.7956445585659515\n",
      "\n",
      "---------------------------DONE FOLD 1 TRAINING--------------------------\n",
      "-------------------------START FOLD 2 TRAINING---------------------------\n",
      "-------------------------START FOLD 2 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186480, number of negative: 626789\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6462\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654332 -> initscore=0.638136\n",
      "[LightGBM] [Info] Start training from score 0.638136\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447703\tvalid_1's binary_logloss: 0.448838\n",
      "[200]\ttraining's binary_logloss: 0.443957\tvalid_1's binary_logloss: 0.446421\n",
      "[300]\ttraining's binary_logloss: 0.441362\tvalid_1's binary_logloss: 0.445252\n",
      "[400]\ttraining's binary_logloss: 0.439114\tvalid_1's binary_logloss: 0.444375\n",
      "[500]\ttraining's binary_logloss: 0.436941\tvalid_1's binary_logloss: 0.443537\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436941\tvalid_1's binary_logloss: 0.443537\n",
      "-------------------------DONE FOLD 2 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8530203191596302 ACC : 0.7965375223077891\n",
      "\n",
      "---------------------------DONE FOLD 2 TRAINING--------------------------\n",
      "-------------------------START FOLD 3 TRAINING---------------------------\n",
      "-------------------------START FOLD 3 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186274, number of negative: 626995\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654218 -> initscore=0.637634\n",
      "[LightGBM] [Info] Start training from score 0.637634\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447817\tvalid_1's binary_logloss: 0.448363\n",
      "[200]\ttraining's binary_logloss: 0.444062\tvalid_1's binary_logloss: 0.446018\n",
      "[300]\ttraining's binary_logloss: 0.441439\tvalid_1's binary_logloss: 0.444803\n",
      "[400]\ttraining's binary_logloss: 0.439148\tvalid_1's binary_logloss: 0.443914\n",
      "[500]\ttraining's binary_logloss: 0.436969\tvalid_1's binary_logloss: 0.443149\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436969\tvalid_1's binary_logloss: 0.443149\n",
      "-------------------------DONE FOLD 3 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8531096957663562 ACC : 0.7976118257201914\n",
      "\n",
      "---------------------------DONE FOLD 3 TRAINING--------------------------\n",
      "-------------------------START FOLD 4 TRAINING---------------------------\n",
      "-------------------------START FOLD 4 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186725, number of negative: 626544\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6464\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654467 -> initscore=0.638734\n",
      "[LightGBM] [Info] Start training from score 0.638734\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447539\tvalid_1's binary_logloss: 0.449312\n",
      "[200]\ttraining's binary_logloss: 0.443752\tvalid_1's binary_logloss: 0.446995\n",
      "[300]\ttraining's binary_logloss: 0.441089\tvalid_1's binary_logloss: 0.445779\n",
      "[400]\ttraining's binary_logloss: 0.438783\tvalid_1's binary_logloss: 0.444916\n",
      "[500]\ttraining's binary_logloss: 0.436798\tvalid_1's binary_logloss: 0.444348\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436798\tvalid_1's binary_logloss: 0.444348\n",
      "-------------------------DONE FOLD 4 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8524413080836457 ACC : 0.7961801564909324\n",
      "\n",
      "---------------------------DONE FOLD 4 TRAINING--------------------------\n",
      "-------------------------START FOLD 5 TRAINING---------------------------\n",
      "-------------------------START FOLD 5 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186937, number of negative: 626332\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6464\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654584 -> initscore=0.639251\n",
      "[LightGBM] [Info] Start training from score 0.639251\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447513\tvalid_1's binary_logloss: 0.449206\n",
      "[200]\ttraining's binary_logloss: 0.443815\tvalid_1's binary_logloss: 0.446952\n",
      "[300]\ttraining's binary_logloss: 0.441151\tvalid_1's binary_logloss: 0.445772\n",
      "[400]\ttraining's binary_logloss: 0.438825\tvalid_1's binary_logloss: 0.444889\n",
      "[500]\ttraining's binary_logloss: 0.436824\tvalid_1's binary_logloss: 0.444285\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436824\tvalid_1's binary_logloss: 0.444285\n",
      "-------------------------DONE FOLD 5 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8523597534206228 ACC : 0.796429430177999\n",
      "\n",
      "---------------------------DONE FOLD 5 TRAINING--------------------------\n"
     ]
    }
   ],
   "source": [
    "# train index 기준 K-fold\n",
    "\n",
    "train = df.copy()\n",
    "predicts_list = list()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "\n",
    "y_train = train['answerCode']\n",
    "train = train.drop(['answerCode'], axis=1)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    kf.split(train)\n",
    "):\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} TRAINING---------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} MODEL LOADING----------------------\"\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    \n",
    "    FEATS = train.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "    FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "    X_train, Y_train = train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    X_valid, Y_valid = train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train[FEATS], Y_train)\n",
    "    lgb_test = lgb.Dataset(X_valid[FEATS], Y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary'}, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"-------------------------DONE FOLD {fold + 1} MODEL LOADING-----------------------\"\n",
    "    )\n",
    "    predicts_list.append(model.predict(test_df[FEATS]))\n",
    "\n",
    "    preds = model.predict(X_valid[FEATS])\n",
    "    acc = accuracy_score(Y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(Y_valid, preds)\n",
    "\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    print(\n",
    "        f\"---------------------------DONE FOLD {fold + 1} TRAINING--------------------------\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "854afb63-8e59-4e78-985d-5258e9a6bd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "def custom_K_fold_5(df): \n",
    "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
    "    random.seed(42)\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    train_data_div_len = 0.2*len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids =[[] for _ in range(5)]\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if sum_of_train_data < train_data_div_len:\n",
    "            user_ids[0].append(user_id)\n",
    "        elif sum_of_train_data < train_data_div_len*2:\n",
    "            user_ids[1].append(user_id)\n",
    "        elif sum_of_train_data < train_data_div_len*3:\n",
    "            user_ids[2].append(user_id)\n",
    "        elif sum_of_train_data < train_data_div_len*4:\n",
    "            user_ids[3].append(user_id)\n",
    "        else:\n",
    "            user_ids[4].append(user_id)\n",
    "            \n",
    "    final_ids =[[] for _ in range(5)]\n",
    "    for i in range(5):\n",
    "        train_idx = [x for x in df['userID'].value_counts().index if x not in user_ids[i]]\n",
    "        final_ids[i].append(train_idx)\n",
    "        final_ids[i].append(user_ids[i])\n",
    "\n",
    "    return final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f47b5a85-abdf-4e9e-8389-319e734cefec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "742dbc57-f6b1-4583-924e-f7a750c297e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6698"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['userID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f48cdf55-a3aa-40ab-9669-40bf86018ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5378 1320\n",
      "5376 1322\n",
      "5363 1335\n",
      "5328 1370\n",
      "5347 1351\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드에서 작동 확인\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    custom_K_fold_5(df)\n",
    "):\n",
    "    print(len(train_idx), len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eaf8ff50-c8c5-4653-b91b-0e947f925dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------START FOLD 1 TRAINING---------------------------\n",
      "-------------------------START FOLD 1 MODEL LOADING----------------------\n",
      "(1813372, 48) (1320, 48)\n",
      "[LightGBM] [Info] Number of positive: 1186899, number of negative: 626473\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6465\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813372, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654526 -> initscore=0.638994\n",
      "[LightGBM] [Info] Start training from score 0.638994\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447853\tvalid_1's binary_logloss: 0.487734\n",
      "[200]\ttraining's binary_logloss: 0.444086\tvalid_1's binary_logloss: 0.48404\n",
      "[300]\ttraining's binary_logloss: 0.44146\tvalid_1's binary_logloss: 0.482124\n",
      "[400]\ttraining's binary_logloss: 0.439106\tvalid_1's binary_logloss: 0.480017\n",
      "[500]\ttraining's binary_logloss: 0.437034\tvalid_1's binary_logloss: 0.478264\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.437034\tvalid_1's binary_logloss: 0.478264\n",
      "-------------------------DONE FOLD 1 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8510989378201922 ACC : 0.7606060606060606\n",
      "\n",
      "---------------------------DONE FOLD 1 TRAINING--------------------------\n",
      "-------------------------START FOLD 2 TRAINING---------------------------\n",
      "-------------------------START FOLD 2 MODEL LOADING----------------------\n",
      "(1813189, 48) (1322, 48)\n",
      "[LightGBM] [Info] Number of positive: 1187062, number of negative: 626127\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6466\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813189, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654682 -> initscore=0.639683\n",
      "[LightGBM] [Info] Start training from score 0.639683\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.44735\tvalid_1's binary_logloss: 0.507465\n",
      "[200]\ttraining's binary_logloss: 0.443495\tvalid_1's binary_logloss: 0.502348\n",
      "[300]\ttraining's binary_logloss: 0.440786\tvalid_1's binary_logloss: 0.499428\n",
      "[400]\ttraining's binary_logloss: 0.438484\tvalid_1's binary_logloss: 0.497662\n",
      "[500]\ttraining's binary_logloss: 0.436398\tvalid_1's binary_logloss: 0.497189\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436398\tvalid_1's binary_logloss: 0.497189\n",
      "-------------------------DONE FOLD 2 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8352089662155944 ACC : 0.7586989409984871\n",
      "\n",
      "---------------------------DONE FOLD 2 TRAINING--------------------------\n",
      "-------------------------START FOLD 3 TRAINING---------------------------\n",
      "-------------------------START FOLD 3 MODEL LOADING----------------------\n",
      "(1813320, 48) (1335, 48)\n",
      "[LightGBM] [Info] Number of positive: 1189078, number of negative: 624242\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813320, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655746 -> initscore=0.644395\n",
      "[LightGBM] [Info] Start training from score 0.644395\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447238\tvalid_1's binary_logloss: 0.514935\n",
      "[200]\ttraining's binary_logloss: 0.443312\tvalid_1's binary_logloss: 0.511291\n",
      "[300]\ttraining's binary_logloss: 0.440435\tvalid_1's binary_logloss: 0.509183\n",
      "[400]\ttraining's binary_logloss: 0.438164\tvalid_1's binary_logloss: 0.509335\n",
      "Early stopping, best iteration is:\n",
      "[384]\ttraining's binary_logloss: 0.438532\tvalid_1's binary_logloss: 0.509112\n",
      "-------------------------DONE FOLD 3 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8263781474820144 ACC : 0.7445692883895131\n",
      "\n",
      "---------------------------DONE FOLD 3 TRAINING--------------------------\n",
      "-------------------------START FOLD 4 TRAINING---------------------------\n",
      "-------------------------START FOLD 4 MODEL LOADING----------------------\n",
      "(1814007, 48) (1370, 48)\n",
      "[LightGBM] [Info] Number of positive: 1181996, number of negative: 632011\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6460\n",
      "[LightGBM] [Info] Number of data points in the train set: 1814007, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.651594 -> initscore=0.626053\n",
      "[LightGBM] [Info] Start training from score 0.626053\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447994\tvalid_1's binary_logloss: 0.518134\n",
      "[200]\ttraining's binary_logloss: 0.443885\tvalid_1's binary_logloss: 0.51549\n",
      "[300]\ttraining's binary_logloss: 0.441258\tvalid_1's binary_logloss: 0.515137\n",
      "[400]\ttraining's binary_logloss: 0.438867\tvalid_1's binary_logloss: 0.514654\n",
      "[500]\ttraining's binary_logloss: 0.436872\tvalid_1's binary_logloss: 0.513904\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436872\tvalid_1's binary_logloss: 0.513904\n",
      "-------------------------DONE FOLD 4 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8245430074870687 ACC : 0.7481751824817519\n",
      "\n",
      "---------------------------DONE FOLD 4 TRAINING--------------------------\n",
      "-------------------------START FOLD 5 TRAINING---------------------------\n",
      "-------------------------START FOLD 5 MODEL LOADING----------------------\n",
      "(1812456, 48) (1351, 48)\n",
      "[LightGBM] [Info] Number of positive: 1187785, number of negative: 624671\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6467\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812456, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655346 -> initscore=0.642620\n",
      "[LightGBM] [Info] Start training from score 0.642620\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.446923\tvalid_1's binary_logloss: 0.509604\n",
      "[200]\ttraining's binary_logloss: 0.442915\tvalid_1's binary_logloss: 0.508069\n",
      "[300]\ttraining's binary_logloss: 0.440315\tvalid_1's binary_logloss: 0.50769\n",
      "[400]\ttraining's binary_logloss: 0.437994\tvalid_1's binary_logloss: 0.507684\n",
      "Early stopping, best iteration is:\n",
      "[315]\ttraining's binary_logloss: 0.439992\tvalid_1's binary_logloss: 0.507372\n",
      "-------------------------DONE FOLD 5 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8289743814704333 ACC : 0.7572168763878608\n",
      "\n",
      "---------------------------DONE FOLD 5 TRAINING--------------------------\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "predicts_list = list()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    custom_K_fold_5(df)\n",
    "):\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} TRAINING---------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} MODEL LOADING----------------------\"\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    \n",
    "    FEATS = train.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "    FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "    train = df.copy()\n",
    "    x_train = train[train['userID'].isin(train_idx)]\n",
    "    x_valid = train[train['userID'].isin(val_idx)]\n",
    "    x_valid = x_valid[x_valid['userID'] != x_valid['userID'].shift(-1)]\n",
    "    X_train, Y_train = x_train.drop(['answerCode'], axis=1), x_train['answerCode']\n",
    "    X_valid, Y_valid = x_valid.drop(['answerCode'], axis=1), x_valid['answerCode']\n",
    "    print(X_train.shape, X_valid.shape)\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train[FEATS], Y_train)\n",
    "    lgb_test = lgb.Dataset(X_valid[FEATS], Y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary'}, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"-------------------------DONE FOLD {fold + 1} MODEL LOADING-----------------------\"\n",
    "    )\n",
    "    predicts_list.append(model.predict(test_df[FEATS]))\n",
    "\n",
    "    preds = model.predict(X_valid[FEATS])\n",
    "    acc = accuracy_score(Y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(Y_valid, preds)\n",
    "\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    print(\n",
    "        f\"---------------------------DONE FOLD {fold + 1} TRAINING--------------------------\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e9d43ba-5e16-4ba8-b63b-bfc9bee0705b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:02<00:00, 320.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# FEATURE ENGINEERING\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "test_df = feature_engineering(test_df)\n",
    "test_df.to_csv(DATA_PATH + 'test_featured.csv', index=False)\n",
    "\n",
    "# Inference\n",
    "test_df = pd.read_csv(DATA_PATH+'test_featured.csv')\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "860b1299-3268-4d6f-a820-6baa72a05f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005697023038779744, 0.9748787404980996)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK PREDICT\n",
    "min(predicts), max(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08924029-7ee2-429f-b080-447203b680a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAKE PREDICTION\n",
    "predicts = np.mean(predicts_list, axis=0)\n",
    "\n",
    "submission = pd.read_csv(DATA_PATH+'/sample_submission.csv')\n",
    "submission['prediction'] = predicts\n",
    "\n",
    "submission.to_csv(DATA_PATH+'/lgbm_kfold_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
