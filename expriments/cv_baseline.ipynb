{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e44cf0b",
   "metadata": {},
   "source": [
    "# CV baseline code and Tuning test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09d4aa4-327c-4149-b592-68ac81c6540b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "import os\n",
    "from math import pi\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "import json\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3a552ff-ae9c-40e2-ac65-f17196e466cf",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61dfd890-482a-41de-8faf-4f4d9dd3ff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    # 문제별 풀이시간\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['diff_Timestamp'] = df['Timestamp'] - df.shift(1)['Timestamp']\n",
    "\n",
    "    testId_df = df[~df.duplicated(['assessmentItemID'])].groupby('testId')\n",
    "    testId2len = {}\n",
    "    for testId, g_df in testId_df:\n",
    "        testId2len[testId] = len(g_df)\n",
    "\n",
    "    userID_df = df.groupby('userID')\n",
    "    start_index_list = []\n",
    "    second_index_list = []\n",
    "\n",
    "    for userID, g_df in tqdm(userID_df):\n",
    "        testId_df = g_df.groupby('testId')\n",
    "        for testId, gg_df in testId_df:\n",
    "            index_list = gg_df.index.tolist()\n",
    "            start_index = 0\n",
    "            if len(gg_df) <= testId2len[testId]:\n",
    "                start_index_list += [index_list[start_index]]\n",
    "                second_index_list += [index_list[start_index + 1]]\n",
    "            else:\n",
    "                div = len(gg_df) // testId2len[testId]\n",
    "                for _ in range(div):\n",
    "                    start_index_list += [index_list[start_index]]\n",
    "                    second_index_list += [index_list[start_index + 1]]\n",
    "                    start_index += testId2len[testId]\n",
    "\n",
    "    df.loc[start_index_list, 'diff_Timestamp'] = df.loc[second_index_list, 'diff_Timestamp'].values\n",
    "    df['elapsed'] = df['diff_Timestamp'].apply(lambda x: x.total_seconds() if not pd.isna(x) else np.nan)\n",
    "\n",
    "\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek # 요일을 숫자로\n",
    "\n",
    "    diff = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff.fillna(pd.Timedelta(seconds=0))\n",
    "    diff = diff['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "    # 문제별 풀이시간\n",
    "    df['elapsed'] = diff\n",
    "    df['elapsed'] = df['elapsed'].apply(lambda x : x if x <650 and x >=0 else 0)\n",
    "\n",
    "    df['testcode']=df['testId'].apply(lambda x : int(x[1:4])//10)\n",
    "    df['problem_number'] = df['assessmentItemID'].apply(lambda x: int(x[7:])) \n",
    "\n",
    "\n",
    "    # feature 별 정답여부\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "    correct_a = df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_a.columns = [\"ass_mean\", 'ass_sum']\n",
    "    correct_p = df.groupby(['problem_number'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_p.columns = [\"prb_mean\", 'prb_sum']\n",
    "    correct_h = df.groupby(['hour'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_h.columns = [\"hour_mean\", 'hour_sum']\n",
    "    correct_d = df.groupby(['dow'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_d.columns = [\"dow_mean\", 'dow_sum'] \n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, correct_a, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, correct_p, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, correct_h, on=['hour'], how=\"left\")\n",
    "    df = pd.merge(df, correct_d, on=['dow'], how=\"left\")\n",
    "\n",
    "\n",
    "    # 정답과 오답 기준으로 나눠서 생각\n",
    "    o_df = df[df['answerCode']==1]\n",
    "    x_df = df[df['answerCode']==0]\n",
    "\n",
    "    elp_k = df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k.columns = ['KnowledgeTag',\"tag_elp\"]\n",
    "    elp_k_o = o_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_o.columns = ['KnowledgeTag', \"tag_elp_o\"]\n",
    "    elp_k_x = x_df.groupby(['KnowledgeTag'])['elapsed'].agg('mean').reset_index()\n",
    "    elp_k_x.columns = ['KnowledgeTag', \"tag_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, elp_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_o, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, elp_k_x, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "    ass_k = df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k.columns = ['assessmentItemID',\"ass_elp\"]\n",
    "    ass_k_o = o_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_o.columns = ['assessmentItemID',\"ass_elp_o\"]\n",
    "    ass_k_x = x_df.groupby(['assessmentItemID'])['elapsed'].agg('mean').reset_index()\n",
    "    ass_k_x.columns = ['assessmentItemID',\"ass_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, ass_k, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_o, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_x, on=['assessmentItemID'], how=\"left\")\n",
    "\n",
    "    prb_k = df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k.columns = ['problem_number',\"prb_elp\"]\n",
    "    prb_k_o = o_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_o.columns = ['problem_number',\"prb_elp_o\"]\n",
    "    prb_k_x = x_df.groupby(['problem_number'])['elapsed'].agg('mean').reset_index()\n",
    "    prb_k_x.columns = ['problem_number',\"prb_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, prb_k, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_o, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_x, on=['problem_number'], how=\"left\")\n",
    "\n",
    "    # 누적합 - 주어진 데이터 이전/이후 데이터들을 포함하는 메모리를 feature로 포함시킴: Sequence Model을 사용하지 않고 일반적인 지도 학습 모델에서 사용하기 위함\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "    df['testcode_o'] = df.groupby(['userID','testcode'])['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeCount'] = df.groupby(['userID','testcode']).cumcount()\n",
    "    df['testcodeAcc'] = df['testcode_o']/df['testcodeCount']\n",
    "    df['tectcodeElp'] = df.groupby(['userID','testcode'])['elapsed'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['testcodeMElp'] = df['tectcodeElp']/df['testcodeCount']\n",
    "\n",
    "\n",
    "\n",
    "    f = lambda x : len(set(x))\n",
    "    t_df = df.groupby(['testId']).agg({\n",
    "    'problem_number':'max',\n",
    "    'KnowledgeTag':f\n",
    "    })\n",
    "    t_df.reset_index(inplace=True)\n",
    "\n",
    "    t_df.columns = ['testId','problem_count',\"tag_count\"]\n",
    "\n",
    "    df = pd.merge(df,t_df,on='testId',how='left')\n",
    "\n",
    "    gdf = df[['userID','testId','problem_number','testcode','Timestamp']].sort_values(by=['userID','testcode','Timestamp'])\n",
    "    gdf['buserID'] = gdf['userID'] != gdf['userID'].shift(1)\n",
    "    gdf['btestcode'] = gdf['testcode'] != gdf['testcode'].shift(1)\n",
    "    gdf['first'] = gdf[['buserID','btestcode']].any(axis=1).apply(lambda x : 1- int(x))\n",
    "    gdf['RepeatedTime'] = gdf['Timestamp'].diff().fillna(pd.Timedelta(seconds=0)) \n",
    "    gdf['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x: x.total_seconds()) * gdf['first']\n",
    "    df['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x : math.log(x+1))\n",
    "\n",
    "    df['prior_KnowledgeTag_frequency'] = df.groupby(['userID','KnowledgeTag']).cumcount()\n",
    "\n",
    "    df['problem_position'] = df['problem_number'] / df[\"problem_count\"]\n",
    "    df['solve_order'] = df.groupby(['userID','testId']).cumcount()\n",
    "    df['solve_order'] = df['solve_order'] - df['problem_count']*(df['solve_order'] > df['problem_count']).apply(int) + 1\n",
    "    df['retest'] = (df['solve_order'] > df['problem_count']).apply(int)\n",
    "    T = df['solve_order'] != df['problem_number']\n",
    "    TT = T.shift(1)\n",
    "    TT[0] = False\n",
    "    df['solved_disorder'] = (TT.apply(lambda x : not x) & T).apply(int)\n",
    "\n",
    "    df['testId'] = df['testId'].apply(lambda x : int(x[1:4]+x[-3]))\n",
    "    df['hour'] = df['Timestamp'].dt.hour\n",
    "    df['dow'] = df['Timestamp'].dt.dayofweek\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96e784d7-9121-45e4-9110-9a93b237db2c",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dadee14f-b7cc-4e98-9014-682eba8f39f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 현제 경로 설정\n",
    "os.chdir('/opt/ml/level2_dkt-recsys-09/DKT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc16ae4-fc7b-481b-89f7-11125d7dd210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [00:20<00:00, 326.75it/s] \n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/opt/ml/input/data'\n",
    "\n",
    "%time\n",
    "dtype = {\n",
    "    'userID': 'int16',\n",
    "    'answerCode': 'int8',\n",
    "    'KnowledgeTag': 'int16'\n",
    "}   \n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "df = df.sort_values(by=['userID', 'Timestamp', 'testId']).reset_index(drop=True)\n",
    "\n",
    "df = feature_engineering(df)\n",
    "df.to_csv(DATA_PATH + 'train_featured.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b9dcbdf-2662-4eb9-b738-e4cacb1c5b1c",
   "metadata": {},
   "source": [
    "## 기본 KFold 모델 적용(userID index 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a62e61b-b53c-44ca-a4b7-183c4e73ee79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------START FOLD 1 TRAINING---------------------------\n",
      "-------------------------START FOLD 1 MODEL LOADING----------------------\n",
      "(1806456, 48) (441630, 48)\n",
      "[LightGBM] [Info] Number of positive: 1181418, number of negative: 625038\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6470\n",
      "[LightGBM] [Info] Number of data points in the train set: 1806456, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653998 -> initscore=0.636658\n",
      "[LightGBM] [Info] Start training from score 0.636658\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.446834\tvalid_1's binary_logloss: 0.450146\n",
      "[200]\ttraining's binary_logloss: 0.442826\tvalid_1's binary_logloss: 0.448247\n",
      "[300]\ttraining's binary_logloss: 0.44008\tvalid_1's binary_logloss: 0.447588\n",
      "[400]\ttraining's binary_logloss: 0.437808\tvalid_1's binary_logloss: 0.447161\n",
      "[500]\ttraining's binary_logloss: 0.435706\tvalid_1's binary_logloss: 0.446944\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.435706\tvalid_1's binary_logloss: 0.446944\n",
      "-------------------------DONE FOLD 1 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8492506490596643 ACC : 0.7947897561306977\n",
      "\n",
      "---------------------------DONE FOLD 1 TRAINING--------------------------\n",
      "-------------------------START FOLD 2 TRAINING---------------------------\n",
      "-------------------------START FOLD 2 MODEL LOADING----------------------\n",
      "(1804088, 48) (443998, 48)\n",
      "[LightGBM] [Info] Number of positive: 1184070, number of negative: 620018\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1804088, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656326 -> initscore=0.646964\n",
      "[LightGBM] [Info] Start training from score 0.646964\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447169\tvalid_1's binary_logloss: 0.449072\n",
      "[200]\ttraining's binary_logloss: 0.443433\tvalid_1's binary_logloss: 0.447529\n",
      "[300]\ttraining's binary_logloss: 0.440797\tvalid_1's binary_logloss: 0.44675\n",
      "[400]\ttraining's binary_logloss: 0.438366\tvalid_1's binary_logloss: 0.446181\n",
      "[500]\ttraining's binary_logloss: 0.436214\tvalid_1's binary_logloss: 0.445973\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436214\tvalid_1's binary_logloss: 0.445973\n",
      "-------------------------DONE FOLD 2 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.852706889694067 ACC : 0.7948031297438277\n",
      "\n",
      "---------------------------DONE FOLD 2 TRAINING--------------------------\n",
      "-------------------------START FOLD 3 TRAINING---------------------------\n",
      "-------------------------START FOLD 3 MODEL LOADING----------------------\n",
      "(1799078, 48) (449008, 48)\n",
      "[LightGBM] [Info] Number of positive: 1181786, number of negative: 617292\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1799078, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656884 -> initscore=0.649440\n",
      "[LightGBM] [Info] Start training from score 0.649440\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.446282\tvalid_1's binary_logloss: 0.453144\n",
      "[200]\ttraining's binary_logloss: 0.442486\tvalid_1's binary_logloss: 0.451278\n",
      "[300]\ttraining's binary_logloss: 0.439729\tvalid_1's binary_logloss: 0.450553\n",
      "[400]\ttraining's binary_logloss: 0.437466\tvalid_1's binary_logloss: 0.450182\n",
      "[500]\ttraining's binary_logloss: 0.435364\tvalid_1's binary_logloss: 0.449902\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.435364\tvalid_1's binary_logloss: 0.449902\n",
      "-------------------------DONE FOLD 3 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8504228251046829 ACC : 0.792865605958023\n",
      "\n",
      "---------------------------DONE FOLD 3 TRAINING--------------------------\n",
      "-------------------------START FOLD 4 TRAINING---------------------------\n",
      "-------------------------START FOLD 4 MODEL LOADING----------------------\n",
      "(1782265, 48) (465821, 48)\n",
      "[LightGBM] [Info] Number of positive: 1167683, number of negative: 614582\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6469\n",
      "[LightGBM] [Info] Number of data points in the train set: 1782265, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655168 -> initscore=0.641834\n",
      "[LightGBM] [Info] Start training from score 0.641834\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447385\tvalid_1's binary_logloss: 0.448851\n",
      "[200]\ttraining's binary_logloss: 0.443332\tvalid_1's binary_logloss: 0.44702\n",
      "[300]\ttraining's binary_logloss: 0.440696\tvalid_1's binary_logloss: 0.446494\n",
      "[400]\ttraining's binary_logloss: 0.438281\tvalid_1's binary_logloss: 0.445959\n",
      "[500]\ttraining's binary_logloss: 0.436131\tvalid_1's binary_logloss: 0.445826\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436131\tvalid_1's binary_logloss: 0.445826\n",
      "-------------------------DONE FOLD 4 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8513649442001504 ACC : 0.7953999497661118\n",
      "\n",
      "---------------------------DONE FOLD 4 TRAINING--------------------------\n",
      "-------------------------START FOLD 5 TRAINING---------------------------\n",
      "-------------------------START FOLD 5 MODEL LOADING----------------------\n",
      "(1800457, 48) (447629, 48)\n",
      "[LightGBM] [Info] Number of positive: 1174663, number of negative: 625794\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6462\n",
      "[LightGBM] [Info] Number of data points in the train set: 1800457, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652425 -> initscore=0.629715\n",
      "[LightGBM] [Info] Start training from score 0.629715\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.448237\tvalid_1's binary_logloss: 0.445221\n",
      "[200]\ttraining's binary_logloss: 0.444305\tvalid_1's binary_logloss: 0.443485\n",
      "[300]\ttraining's binary_logloss: 0.441635\tvalid_1's binary_logloss: 0.442884\n",
      "[400]\ttraining's binary_logloss: 0.43924\tvalid_1's binary_logloss: 0.442502\n",
      "[500]\ttraining's binary_logloss: 0.437114\tvalid_1's binary_logloss: 0.442321\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.437114\tvalid_1's binary_logloss: 0.442321\n",
      "-------------------------DONE FOLD 5 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.849856271220662 ACC : 0.7973969514933126\n",
      "\n",
      "---------------------------DONE FOLD 5 TRAINING--------------------------\n"
     ]
    }
   ],
   "source": [
    "# userID index 기준 K-fold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train = df.copy()\n",
    "predicts_list = list()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    kf.split(train[\"userID\"].unique().tolist())\n",
    "):\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} TRAINING---------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} MODEL LOADING----------------------\"\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    \n",
    "    FEATS = train.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "    FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "    train = df.copy()\n",
    "    x_train = train[train['userID'].isin(train_idx)]\n",
    "    x_valid = train[train['userID'].isin(val_idx)]\n",
    "    X_train, Y_train = x_train.drop(['answerCode'], axis=1), x_train['answerCode']\n",
    "    X_valid, Y_valid = x_valid.drop(['answerCode'], axis=1), x_valid['answerCode']\n",
    "    # print(X_train.shape, X_valid.shape)\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train[FEATS], Y_train)\n",
    "    lgb_test = lgb.Dataset(X_valid[FEATS], Y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary'}, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"-------------------------DONE FOLD {fold + 1} MODEL LOADING-----------------------\"\n",
    "    )\n",
    "    predicts_list.append(model.predict(test_df[FEATS]))\n",
    "\n",
    "    preds = model.predict(X_valid[FEATS])\n",
    "    acc = accuracy_score(Y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(Y_valid, preds)\n",
    "\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    print(\n",
    "        f\"---------------------------DONE FOLD {fold + 1} TRAINING--------------------------\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa238bb-d8f8-44fe-a5e5-8fa022732ed5",
   "metadata": {},
   "source": [
    "## 기본 KFold 모델 적용(train index 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4b7159c-22f7-43de-963f-3126976e4306",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------START FOLD 1 TRAINING---------------------------\n",
      "-------------------------START FOLD 1 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186404, number of negative: 626864\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813268, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654290 -> initscore=0.637953\n",
      "[LightGBM] [Info] Start training from score 0.637953\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447301\tvalid_1's binary_logloss: 0.449423\n",
      "[200]\ttraining's binary_logloss: 0.443451\tvalid_1's binary_logloss: 0.44703\n",
      "[300]\ttraining's binary_logloss: 0.440961\tvalid_1's binary_logloss: 0.446065\n",
      "[400]\ttraining's binary_logloss: 0.438722\tvalid_1's binary_logloss: 0.445244\n",
      "[500]\ttraining's binary_logloss: 0.436591\tvalid_1's binary_logloss: 0.444539\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436591\tvalid_1's binary_logloss: 0.444539\n",
      "-------------------------DONE FOLD 1 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8521355356441817 ACC : 0.7956445585659515\n",
      "\n",
      "---------------------------DONE FOLD 1 TRAINING--------------------------\n",
      "-------------------------START FOLD 2 TRAINING---------------------------\n",
      "-------------------------START FOLD 2 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186480, number of negative: 626789\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6462\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654332 -> initscore=0.638136\n",
      "[LightGBM] [Info] Start training from score 0.638136\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447703\tvalid_1's binary_logloss: 0.448838\n",
      "[200]\ttraining's binary_logloss: 0.443957\tvalid_1's binary_logloss: 0.446421\n",
      "[300]\ttraining's binary_logloss: 0.441362\tvalid_1's binary_logloss: 0.445252\n",
      "[400]\ttraining's binary_logloss: 0.439114\tvalid_1's binary_logloss: 0.444375\n",
      "[500]\ttraining's binary_logloss: 0.436941\tvalid_1's binary_logloss: 0.443537\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436941\tvalid_1's binary_logloss: 0.443537\n",
      "-------------------------DONE FOLD 2 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8530203191596302 ACC : 0.7965375223077891\n",
      "\n",
      "---------------------------DONE FOLD 2 TRAINING--------------------------\n",
      "-------------------------START FOLD 3 TRAINING---------------------------\n",
      "-------------------------START FOLD 3 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186274, number of negative: 626995\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654218 -> initscore=0.637634\n",
      "[LightGBM] [Info] Start training from score 0.637634\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447817\tvalid_1's binary_logloss: 0.448363\n",
      "[200]\ttraining's binary_logloss: 0.444062\tvalid_1's binary_logloss: 0.446018\n",
      "[300]\ttraining's binary_logloss: 0.441439\tvalid_1's binary_logloss: 0.444803\n",
      "[400]\ttraining's binary_logloss: 0.439148\tvalid_1's binary_logloss: 0.443914\n",
      "[500]\ttraining's binary_logloss: 0.436969\tvalid_1's binary_logloss: 0.443149\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436969\tvalid_1's binary_logloss: 0.443149\n",
      "-------------------------DONE FOLD 3 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8531096957663562 ACC : 0.7976118257201914\n",
      "\n",
      "---------------------------DONE FOLD 3 TRAINING--------------------------\n",
      "-------------------------START FOLD 4 TRAINING---------------------------\n",
      "-------------------------START FOLD 4 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186725, number of negative: 626544\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6464\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654467 -> initscore=0.638734\n",
      "[LightGBM] [Info] Start training from score 0.638734\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447539\tvalid_1's binary_logloss: 0.449312\n",
      "[200]\ttraining's binary_logloss: 0.443752\tvalid_1's binary_logloss: 0.446995\n",
      "[300]\ttraining's binary_logloss: 0.441089\tvalid_1's binary_logloss: 0.445779\n",
      "[400]\ttraining's binary_logloss: 0.438783\tvalid_1's binary_logloss: 0.444916\n",
      "[500]\ttraining's binary_logloss: 0.436798\tvalid_1's binary_logloss: 0.444348\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436798\tvalid_1's binary_logloss: 0.444348\n",
      "-------------------------DONE FOLD 4 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8524413080836457 ACC : 0.7961801564909324\n",
      "\n",
      "---------------------------DONE FOLD 4 TRAINING--------------------------\n",
      "-------------------------START FOLD 5 TRAINING---------------------------\n",
      "-------------------------START FOLD 5 MODEL LOADING----------------------\n",
      "[LightGBM] [Info] Number of positive: 1186937, number of negative: 626332\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6464\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813269, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654584 -> initscore=0.639251\n",
      "[LightGBM] [Info] Start training from score 0.639251\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447513\tvalid_1's binary_logloss: 0.449206\n",
      "[200]\ttraining's binary_logloss: 0.443815\tvalid_1's binary_logloss: 0.446952\n",
      "[300]\ttraining's binary_logloss: 0.441151\tvalid_1's binary_logloss: 0.445772\n",
      "[400]\ttraining's binary_logloss: 0.438825\tvalid_1's binary_logloss: 0.444889\n",
      "[500]\ttraining's binary_logloss: 0.436824\tvalid_1's binary_logloss: 0.444285\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436824\tvalid_1's binary_logloss: 0.444285\n",
      "-------------------------DONE FOLD 5 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8523597534206228 ACC : 0.796429430177999\n",
      "\n",
      "---------------------------DONE FOLD 5 TRAINING--------------------------\n"
     ]
    }
   ],
   "source": [
    "# train index 기준 K-fold\n",
    "\n",
    "train = df.copy()\n",
    "predicts_list = list()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "\n",
    "y_train = train['answerCode']\n",
    "train = train.drop(['answerCode'], axis=1)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    kf.split(train)\n",
    "):\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} TRAINING---------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} MODEL LOADING----------------------\"\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    \n",
    "    FEATS = train.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "    FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "    X_train, Y_train = train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    X_valid, Y_valid = train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train[FEATS], Y_train)\n",
    "    lgb_test = lgb.Dataset(X_valid[FEATS], Y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary'}, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"-------------------------DONE FOLD {fold + 1} MODEL LOADING-----------------------\"\n",
    "    )\n",
    "    predicts_list.append(model.predict(test_df[FEATS]))\n",
    "\n",
    "    preds = model.predict(X_valid[FEATS])\n",
    "    acc = accuracy_score(Y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(Y_valid, preds)\n",
    "\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    print(\n",
    "        f\"---------------------------DONE FOLD {fold + 1} TRAINING--------------------------\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be0e813-f181-45ef-9482-5cd836d9efd7",
   "metadata": {},
   "source": [
    "## Custom K-Fold 모델 적용(userID index 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "854afb63-8e59-4e78-985d-5258e9a6bd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "def custom_K_fold_5(df): \n",
    "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
    "    random.seed(42)\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    train_data_div_len = 0.2*len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids =[[] for _ in range(5)]\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if sum_of_train_data < train_data_div_len:\n",
    "            user_ids[0].append(user_id)\n",
    "        elif sum_of_train_data < train_data_div_len*2:\n",
    "            user_ids[1].append(user_id)\n",
    "        elif sum_of_train_data < train_data_div_len*3:\n",
    "            user_ids[2].append(user_id)\n",
    "        elif sum_of_train_data < train_data_div_len*4:\n",
    "            user_ids[3].append(user_id)\n",
    "        else:\n",
    "            user_ids[4].append(user_id)\n",
    "            \n",
    "    final_ids =[[] for _ in range(5)]\n",
    "    for i in range(5):\n",
    "        train_idx = [x for x in df['userID'].value_counts().index if x not in user_ids[i]]\n",
    "        final_ids[i].append(train_idx)\n",
    "        final_ids[i].append(user_ids[i])\n",
    "\n",
    "    return final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f47b5a85-abdf-4e9e-8389-319e734cefec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "742dbc57-f6b1-4583-924e-f7a750c297e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6698"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['userID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f48cdf55-a3aa-40ab-9669-40bf86018ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5378 1320\n",
      "5376 1322\n",
      "5363 1335\n",
      "5328 1370\n",
      "5347 1351\n"
     ]
    }
   ],
   "source": [
    "# 기존 코드에서 작동 확인\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    custom_K_fold_5(df)\n",
    "):\n",
    "    print(len(train_idx), len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eaf8ff50-c8c5-4653-b91b-0e947f925dc0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------START FOLD 1 TRAINING---------------------------\n",
      "-------------------------START FOLD 1 MODEL LOADING----------------------\n",
      "(1813372, 48) (1320, 48)\n",
      "[LightGBM] [Info] Number of positive: 1186899, number of negative: 626473\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6465\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813372, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654526 -> initscore=0.638994\n",
      "[LightGBM] [Info] Start training from score 0.638994\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447853\tvalid_1's binary_logloss: 0.487734\n",
      "[200]\ttraining's binary_logloss: 0.444086\tvalid_1's binary_logloss: 0.48404\n",
      "[300]\ttraining's binary_logloss: 0.44146\tvalid_1's binary_logloss: 0.482124\n",
      "[400]\ttraining's binary_logloss: 0.439106\tvalid_1's binary_logloss: 0.480017\n",
      "[500]\ttraining's binary_logloss: 0.437034\tvalid_1's binary_logloss: 0.478264\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.437034\tvalid_1's binary_logloss: 0.478264\n",
      "-------------------------DONE FOLD 1 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8510989378201922 ACC : 0.7606060606060606\n",
      "\n",
      "---------------------------DONE FOLD 1 TRAINING--------------------------\n",
      "-------------------------START FOLD 2 TRAINING---------------------------\n",
      "-------------------------START FOLD 2 MODEL LOADING----------------------\n",
      "(1813189, 48) (1322, 48)\n",
      "[LightGBM] [Info] Number of positive: 1187062, number of negative: 626127\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6466\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813189, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654682 -> initscore=0.639683\n",
      "[LightGBM] [Info] Start training from score 0.639683\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.44735\tvalid_1's binary_logloss: 0.507465\n",
      "[200]\ttraining's binary_logloss: 0.443495\tvalid_1's binary_logloss: 0.502348\n",
      "[300]\ttraining's binary_logloss: 0.440786\tvalid_1's binary_logloss: 0.499428\n",
      "[400]\ttraining's binary_logloss: 0.438484\tvalid_1's binary_logloss: 0.497662\n",
      "[500]\ttraining's binary_logloss: 0.436398\tvalid_1's binary_logloss: 0.497189\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436398\tvalid_1's binary_logloss: 0.497189\n",
      "-------------------------DONE FOLD 2 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8352089662155944 ACC : 0.7586989409984871\n",
      "\n",
      "---------------------------DONE FOLD 2 TRAINING--------------------------\n",
      "-------------------------START FOLD 3 TRAINING---------------------------\n",
      "-------------------------START FOLD 3 MODEL LOADING----------------------\n",
      "(1813320, 48) (1335, 48)\n",
      "[LightGBM] [Info] Number of positive: 1189078, number of negative: 624242\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6468\n",
      "[LightGBM] [Info] Number of data points in the train set: 1813320, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655746 -> initscore=0.644395\n",
      "[LightGBM] [Info] Start training from score 0.644395\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447238\tvalid_1's binary_logloss: 0.514935\n",
      "[200]\ttraining's binary_logloss: 0.443312\tvalid_1's binary_logloss: 0.511291\n",
      "[300]\ttraining's binary_logloss: 0.440435\tvalid_1's binary_logloss: 0.509183\n",
      "[400]\ttraining's binary_logloss: 0.438164\tvalid_1's binary_logloss: 0.509335\n",
      "Early stopping, best iteration is:\n",
      "[384]\ttraining's binary_logloss: 0.438532\tvalid_1's binary_logloss: 0.509112\n",
      "-------------------------DONE FOLD 3 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8263781474820144 ACC : 0.7445692883895131\n",
      "\n",
      "---------------------------DONE FOLD 3 TRAINING--------------------------\n",
      "-------------------------START FOLD 4 TRAINING---------------------------\n",
      "-------------------------START FOLD 4 MODEL LOADING----------------------\n",
      "(1814007, 48) (1370, 48)\n",
      "[LightGBM] [Info] Number of positive: 1181996, number of negative: 632011\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6460\n",
      "[LightGBM] [Info] Number of data points in the train set: 1814007, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.651594 -> initscore=0.626053\n",
      "[LightGBM] [Info] Start training from score 0.626053\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.447994\tvalid_1's binary_logloss: 0.518134\n",
      "[200]\ttraining's binary_logloss: 0.443885\tvalid_1's binary_logloss: 0.51549\n",
      "[300]\ttraining's binary_logloss: 0.441258\tvalid_1's binary_logloss: 0.515137\n",
      "[400]\ttraining's binary_logloss: 0.438867\tvalid_1's binary_logloss: 0.514654\n",
      "[500]\ttraining's binary_logloss: 0.436872\tvalid_1's binary_logloss: 0.513904\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.436872\tvalid_1's binary_logloss: 0.513904\n",
      "-------------------------DONE FOLD 4 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8245430074870687 ACC : 0.7481751824817519\n",
      "\n",
      "---------------------------DONE FOLD 4 TRAINING--------------------------\n",
      "-------------------------START FOLD 5 TRAINING---------------------------\n",
      "-------------------------START FOLD 5 MODEL LOADING----------------------\n",
      "(1812456, 48) (1351, 48)\n",
      "[LightGBM] [Info] Number of positive: 1187785, number of negative: 624671\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6467\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812456, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655346 -> initscore=0.642620\n",
      "[LightGBM] [Info] Start training from score 0.642620\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.446923\tvalid_1's binary_logloss: 0.509604\n",
      "[200]\ttraining's binary_logloss: 0.442915\tvalid_1's binary_logloss: 0.508069\n",
      "[300]\ttraining's binary_logloss: 0.440315\tvalid_1's binary_logloss: 0.50769\n",
      "[400]\ttraining's binary_logloss: 0.437994\tvalid_1's binary_logloss: 0.507684\n",
      "Early stopping, best iteration is:\n",
      "[315]\ttraining's binary_logloss: 0.439992\tvalid_1's binary_logloss: 0.507372\n",
      "-------------------------DONE FOLD 5 MODEL LOADING-----------------------\n",
      "VALID AUC : 0.8289743814704333 ACC : 0.7572168763878608\n",
      "\n",
      "---------------------------DONE FOLD 5 TRAINING--------------------------\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "predicts_list = list()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(\n",
    "    custom_K_fold_5(df)\n",
    "):\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} TRAINING---------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-------------------------START FOLD {fold + 1} MODEL LOADING----------------------\"\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    \n",
    "    FEATS = train.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "    FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "    train = df.copy()\n",
    "    x_train = train[train['userID'].isin(train_idx)]\n",
    "    x_valid = train[train['userID'].isin(val_idx)]\n",
    "    x_valid = x_valid[x_valid['userID'] != x_valid['userID'].shift(-1)]\n",
    "    X_train, Y_train = x_train.drop(['answerCode'], axis=1), x_train['answerCode']\n",
    "    X_valid, Y_valid = x_valid.drop(['answerCode'], axis=1), x_valid['answerCode']\n",
    "    print(X_train.shape, X_valid.shape)\n",
    "\n",
    "    # Create the LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train[FEATS], Y_train)\n",
    "    lgb_test = lgb.Dataset(X_valid[FEATS], Y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        {'objective': 'binary'}, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"-------------------------DONE FOLD {fold + 1} MODEL LOADING-----------------------\"\n",
    "    )\n",
    "    predicts_list.append(model.predict(test_df[FEATS]))\n",
    "\n",
    "    preds = model.predict(X_valid[FEATS])\n",
    "    acc = accuracy_score(Y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(Y_valid, preds)\n",
    "\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    print(\n",
    "        f\"---------------------------DONE FOLD {fold + 1} TRAINING--------------------------\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a0c0373-91e6-4f8c-8b9d-4a893cdd07b0",
   "metadata": {},
   "source": [
    "## test data 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e9d43ba-5e16-4ba8-b63b-bfc9bee0705b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:02<00:00, 320.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# FEATURE ENGINEERING\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "test_df = feature_engineering(test_df)\n",
    "test_df.to_csv(DATA_PATH + 'test_featured.csv', index=False)\n",
    "\n",
    "# Inference\n",
    "test_df = pd.read_csv(DATA_PATH+'test_featured.csv')\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "860b1299-3268-4d6f-a820-6baa72a05f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005697023038779744, 0.9748787404980996)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK PREDICT\n",
    "min(predicts), max(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08924029-7ee2-429f-b080-447203b680a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAKE PREDICTION\n",
    "predicts = np.mean(predicts_list, axis=0)\n",
    "\n",
    "submission = pd.read_csv(DATA_PATH+'/sample_submission.csv')\n",
    "submission['prediction'] = predicts\n",
    "\n",
    "submission.to_csv(DATA_PATH+'/lgbm_kfold_submission.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0bf232f",
   "metadata": {},
   "source": [
    "# Optuna + FE 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06874e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/opt/ml/input/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b211c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH+'/train_data.csv')\n",
    "df[\"Timestamp\"] = df[\"Timestamp\"].apply(convert_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a2bf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41932b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "random.seed(42)\n",
    "def custom_train_test_split(df, ratio=0.8, split=True):\n",
    "    \n",
    "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
    "    random.shuffle(users)\n",
    "    \n",
    "    max_train_data_len = ratio*len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids =[]\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if max_train_data_len < sum_of_train_data:\n",
    "            break\n",
    "        user_ids.append(user_id)\n",
    "\n",
    "\n",
    "    train = df[df['userID'].isin(user_ids)]\n",
    "    test = df[df['userID'].isin(user_ids) == False]\n",
    "\n",
    "    #test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "    test = test[test['userID'] != test['userID'].shift(-1)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee91b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df = df.sort_values(['userID', 'Timestamp'])\n",
    "\n",
    "    # diff\n",
    "    df['diff'] = df.sort_values(['userID','Timestamp']).groupby('userID')['Timestamp'].diff()\n",
    "\n",
    "    diff_df = df['diff']\n",
    "    diff_df.dropna(inplace=True)\n",
    "\n",
    "    # nan은 -1\n",
    "    # 600(10분) 이상이면 다 600\n",
    "    df['diff'].fillna(-1, inplace=True)\n",
    "    idx = df[df['diff'] >= 600].index\n",
    "    df.loc[idx, 'diff'] = 600\n",
    "\n",
    "    tmp = df[df['diff'] >= 0]\n",
    "    correct_k = tmp.groupby(['KnowledgeTag'])['diff'].agg(['mean'])\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "\n",
    "    df.sort_values(by=['userID','Timestamp'], inplace=True)\n",
    "\n",
    "    #유저들의 문제 풀이수, 정답 수, 정답률\n",
    "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
    "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
    "\n",
    "    #testId와 KnowledgeTag의 전체 정답률\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "\n",
    "    df['hour'] = pd.to_datetime(df['Timestamp']).dt.hour\n",
    "    df['dow'] = pd.to_datetime(df['Timestamp']).dt.dayofweek # 요일을 숫자로\n",
    "\n",
    "    df['testcode']=df['testId'].apply(lambda x : int(x[1:4])//10)\n",
    "    df['problem_number'] = df['assessmentItemID'].apply(lambda x: int(x[7:])) \n",
    "\n",
    "    # feature 별 정답여부\n",
    "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
    "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
    "    correct_a = df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_a.columns = [\"ass_mean\", 'ass_sum']\n",
    "    correct_p = df.groupby(['problem_number'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_p.columns = [\"prb_mean\", 'prb_sum']\n",
    "    correct_h = df.groupby(['hour'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_h.columns = [\"hour_mean\", 'hour_sum']\n",
    "    correct_d = df.groupby(['dow'])['answerCode'].agg(['mean', 'sum'])\n",
    "    correct_d.columns = [\"dow_mean\", 'dow_sum'] \n",
    "\n",
    "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, correct_a, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, correct_p, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, correct_h, on=['hour'], how=\"left\")\n",
    "    df = pd.merge(df, correct_d, on=['dow'], how=\"left\")\n",
    "\n",
    "\n",
    "    f = lambda x : len(set(x))\n",
    "    t_df = df.groupby(['testId']).agg({\n",
    "    'problem_number':'max',\n",
    "    'KnowledgeTag':f\n",
    "    })\n",
    "    t_df.reset_index(inplace=True)\n",
    "\n",
    "    t_df.columns = ['testId','problem_count',\"tag_count\"]\n",
    "\n",
    "    df = pd.merge(df,t_df,on='testId',how='left')\n",
    "\n",
    "    gdf = df[['userID','testId','problem_number','testcode','Timestamp']].sort_values(by=['userID','testcode','Timestamp'])\n",
    "    gdf['buserID'] = gdf['userID'] != gdf['userID'].shift(1)\n",
    "    gdf['btestcode'] = gdf['testcode'] != gdf['testcode'].shift(1)\n",
    "    gdf['first'] = gdf[['buserID','btestcode']].any(axis=1).apply(lambda x : 1- int(x))\n",
    "    gdf['RepeatedTime'] = pd.to_datetime(gdf['Timestamp']).diff().fillna(pd.Timedelta(seconds=0)) \n",
    "    gdf['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x: x.total_seconds()) * gdf['first']\n",
    "    df['RepeatedTime'] = gdf['RepeatedTime'].apply(lambda x : math.log(x+1))\n",
    "\n",
    "    df['prior_KnowledgeTag_frequency'] = df.groupby(['userID','KnowledgeTag']).cumcount()\n",
    "\n",
    "    df['problem_position'] = df['problem_number'] / df[\"problem_count\"]\n",
    "    df['solve_order'] = df.groupby(['userID','testId']).cumcount()\n",
    "    df['solve_order'] = df['solve_order'] - df['problem_count']*(df['solve_order'] > df['problem_count']).apply(int) + 1\n",
    "    df['retest'] = (df['solve_order'] > df['problem_count']).apply(int)\n",
    "    T = df['solve_order'] != df['problem_number']\n",
    "    TT = T.shift(1)\n",
    "    TT[0] = False\n",
    "    df['solved_disorder'] = (TT.apply(lambda x : not x) & T).apply(int)\n",
    "\n",
    "    df['testId'] = df['testId'].apply(lambda x : int(x[1:4]+x[-3]))\n",
    "\n",
    "    # 정답과 오답 기준으로 나눠서 생각\n",
    "    o_df = df[df['answerCode']==1]\n",
    "    x_df = df[df['answerCode']==0]\n",
    "\n",
    "    diff_k = df.groupby(['KnowledgeTag'])['diff'].agg('mean').reset_index()\n",
    "    diff_k.columns = ['KnowledgeTag',\"tag_diff\"]\n",
    "    diff_k_o = o_df.groupby(['KnowledgeTag'])['diff'].agg('mean').reset_index()\n",
    "    diff_k_o.columns = ['KnowledgeTag', \"tag_diff_o\"]\n",
    "    diff_k_x = x_df.groupby(['KnowledgeTag'])['diff'].agg('mean').reset_index()\n",
    "    diff_k_x.columns = ['KnowledgeTag', \"tag_diff_x\"]\n",
    "\n",
    "    df = pd.merge(df, diff_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, diff_k_o, on=['KnowledgeTag'], how=\"left\")\n",
    "    df = pd.merge(df, diff_k_x, on=['KnowledgeTag'], how=\"left\")\n",
    "\n",
    "    ass_k = df.groupby(['assessmentItemID'])['diff'].agg('mean').reset_index()\n",
    "    ass_k.columns = ['assessmentItemID',\"ass_diff\"]\n",
    "    ass_k_o = o_df.groupby(['assessmentItemID'])['diff'].agg('mean').reset_index()\n",
    "    ass_k_o.columns = ['assessmentItemID',\"ass_diff_o\"]\n",
    "    ass_k_x = x_df.groupby(['assessmentItemID'])['diff'].agg('mean').reset_index()\n",
    "    ass_k_x.columns = ['assessmentItemID',\"ass_diff_x\"]\n",
    "\n",
    "    df = pd.merge(df, ass_k, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_o, on=['assessmentItemID'], how=\"left\")\n",
    "    df = pd.merge(df, ass_k_x, on=['assessmentItemID'], how=\"left\")\n",
    "\n",
    "    prb_k = df.groupby(['problem_number'])['diff'].agg('mean').reset_index()\n",
    "    prb_k.columns = ['problem_number',\"prb_diff\"]\n",
    "    prb_k_o = o_df.groupby(['problem_number'])['diff'].agg('mean').reset_index()\n",
    "    prb_k_o.columns = ['problem_number',\"prb_diff_o\"]\n",
    "    prb_k_x = x_df.groupby(['problem_number'])['diff'].agg('mean').reset_index()\n",
    "    prb_k_x.columns = ['problem_number',\"prb_diff_x\"]\n",
    "\n",
    "    df = pd.merge(df, prb_k, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_o, on=['problem_number'], how=\"left\")\n",
    "    df = pd.merge(df, prb_k_x, on=['problem_number'], how=\"left\")\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffddac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_label_encoding(df, is_train=True):\n",
    "    cate_cols = [\"assessmentItemID\", \"testId\", \"KnowledgeTag\"]\n",
    "\n",
    "    if not os.path.exists('asset/'):\n",
    "        os.makedirs('asset/')    \n",
    "\n",
    "    for col in cate_cols:\n",
    "        le = LabelEncoder()\n",
    "        if is_train:\n",
    "            # For UNKNOWN class\n",
    "            a = df[col].unique().tolist() + [\"unknown\"]\n",
    "            le.fit(a)\n",
    "            le_path = os.path.join('asset/', col + \"_classes.npy\")            \n",
    "            np.save(le_path, le.classes_)\n",
    "        else:\n",
    "            label_path = os.path.join('asset/', col + \"_classes.npy\")\n",
    "            le.classes_ = np.load(label_path)\n",
    "            df[col] = df[col].apply(lambda x: x if str(x) in le.classes_ else \"unknown\")\n",
    "\n",
    "        # 모든 컬럼이 범주형이라고 가정\n",
    "        df[col] = df[col].astype(str)\n",
    "        test = le.transform(df[col])\n",
    "        df[col] = test\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f5cb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(s):\n",
    "     timestamp = time.mktime(datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())\n",
    "     return int(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caaade37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df)\n",
    "df = categorical_label_encoding(df, is_train=True) # LGBM을 위한 FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f93f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS = ['userID', 'user_acc', 'user_correct_answer', 'diff', 'ass_diff_o', 'ass_diff_x', 'ass_mean', 'assessmentItemID', 'ass_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84c0814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATS = df.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "'Timestamp' in FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d41c188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별 분리\n",
    "train, test = custom_train_test_split(df)\n",
    "\n",
    "# 사용할 Feature 설정\n",
    "# FEATS = df.select_dtypes(include=[\"int\", \"int8\", \"int16\", \"int64\", \"float\", \"float16\", \"float64\"]).columns\n",
    "# FEATS = [col for col in FEATS if col not in ['answerCode']]\n",
    "\n",
    "# X, y 값 분리\n",
    "y_train = train['answerCode']\n",
    "train = train.drop(['answerCode'], axis=1)\n",
    "\n",
    "y_test = test['answerCode']\n",
    "test = test.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a7c42f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:50:54,649]\u001b[0m A new study created in memory with name: no-name-98670657-5673-41ea-9419-7e94715bece5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.819778\tvalid_1's auc: 0.797107\n",
      "[200]\ttraining's auc: 0.821841\tvalid_1's auc: 0.799375\n",
      "[300]\ttraining's auc: 0.823583\tvalid_1's auc: 0.801126\n",
      "[400]\ttraining's auc: 0.825047\tvalid_1's auc: 0.802498\n",
      "[500]\ttraining's auc: 0.826309\tvalid_1's auc: 0.803791\n",
      "[600]\ttraining's auc: 0.827402\tvalid_1's auc: 0.804865\n",
      "[700]\ttraining's auc: 0.828289\tvalid_1's auc: 0.805635\n",
      "[800]\ttraining's auc: 0.829056\tvalid_1's auc: 0.806366\n",
      "[900]\ttraining's auc: 0.82974\tvalid_1's auc: 0.806856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:53:18,452]\u001b[0m Trial 0 finished with value: 0.8071322122460997 and parameters: {'num_leaves': 62, 'learning_rate': 0.005492274570909453, 'feature_fraction': 0.9013191926640104, 'bagging_fraction': 0.6867032081165119, 'bagging_freq': 5, 'lambda_l1': 0.0002321004478099803, 'lambda_l2': 2.5998310603506103}. Best is trial 0 with value: 0.8071322122460997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.830336\tvalid_1's auc: 0.807132\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.830336\tvalid_1's auc: 0.807132\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.823475\tvalid_1's auc: 0.798471\n",
      "[200]\ttraining's auc: 0.827165\tvalid_1's auc: 0.801709\n",
      "[300]\ttraining's auc: 0.830206\tvalid_1's auc: 0.804632\n",
      "[400]\ttraining's auc: 0.832585\tvalid_1's auc: 0.806235\n",
      "[500]\ttraining's auc: 0.834662\tvalid_1's auc: 0.80768\n",
      "[600]\ttraining's auc: 0.836696\tvalid_1's auc: 0.80879\n",
      "[700]\ttraining's auc: 0.83847\tvalid_1's auc: 0.809421\n",
      "[800]\ttraining's auc: 0.839957\tvalid_1's auc: 0.809878\n",
      "[900]\ttraining's auc: 0.841346\tvalid_1's auc: 0.809894\n",
      "[1000]\ttraining's auc: 0.842561\tvalid_1's auc: 0.810115\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.842561\tvalid_1's auc: 0.810115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:56:02,568]\u001b[0m Trial 1 finished with value: 0.8101150831616132 and parameters: {'num_leaves': 301, 'learning_rate': 0.008919553725370155, 'feature_fraction': 0.5822188695978939, 'bagging_fraction': 0.8825907090782631, 'bagging_freq': 9, 'lambda_l1': 1.1369606054802198e-06, 'lambda_l2': 0.005318865827399402}. Best is trial 1 with value: 0.8101150831616132.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.787014\tvalid_1's auc: 0.753015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:56:29,771]\u001b[0m Trial 2 finished with value: 0.7569431667647923 and parameters: {'num_leaves': 404, 'learning_rate': 0.0013845921382871654, 'feature_fraction': 0.12306228070881381, 'bagging_fraction': 0.37862222403487855, 'bagging_freq': 9, 'lambda_l1': 1.199491308438662e-08, 'lambda_l2': 1.7666670550191215e-06}. Best is trial 1 with value: 0.8101150831616132.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\ttraining's auc: 0.790139\tvalid_1's auc: 0.756943\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.789729\tvalid_1's auc: 0.755835\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:56:58,806]\u001b[0m Trial 3 finished with value: 0.7587255298793054 and parameters: {'num_leaves': 697, 'learning_rate': 0.00984275396879612, 'feature_fraction': 0.11472126988488936, 'bagging_fraction': 0.8590865224756986, 'bagging_freq': 7, 'lambda_l1': 5.304526100312594e-07, 'lambda_l2': 0.012520016697451624}. Best is trial 1 with value: 0.8101150831616132.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[79]\ttraining's auc: 0.791925\tvalid_1's auc: 0.758726\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.837274\tvalid_1's auc: 0.810258\n",
      "[200]\ttraining's auc: 0.846384\tvalid_1's auc: 0.81193\n",
      "[300]\ttraining's auc: 0.853859\tvalid_1's auc: 0.810957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:57:54,963]\u001b[0m Trial 4 finished with value: 0.8122010229614366 and parameters: {'num_leaves': 614, 'learning_rate': 0.028574283104417796, 'feature_fraction': 0.8563348902225885, 'bagging_fraction': 0.5674193033202796, 'bagging_freq': 5, 'lambda_l1': 0.00041591967741612135, 'lambda_l2': 3.271801911135647e-07}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's auc: 0.847543\tvalid_1's auc: 0.812201\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.84017\tvalid_1's auc: 0.80707\n",
      "[200]\ttraining's auc: 0.851579\tvalid_1's auc: 0.809317\n",
      "[300]\ttraining's auc: 0.860819\tvalid_1's auc: 0.810101\n",
      "[400]\ttraining's auc: 0.868234\tvalid_1's auc: 0.810623\n",
      "Early stopping, best iteration is:\n",
      "[375]\ttraining's auc: 0.866308\tvalid_1's auc: 0.811026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 07:59:18,980]\u001b[0m Trial 5 finished with value: 0.811025813217545 and parameters: {'num_leaves': 739, 'learning_rate': 0.04802725439378667, 'feature_fraction': 0.47315137495624715, 'bagging_fraction': 0.7991643468056636, 'bagging_freq': 7, 'lambda_l1': 6.899385766876918e-05, 'lambda_l2': 1.3614325829864167e-06}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.798249\tvalid_1's auc: 0.766257\n",
      "[200]\ttraining's auc: 0.803984\tvalid_1's auc: 0.773212\n",
      "[300]\ttraining's auc: 0.80931\tvalid_1's auc: 0.780661\n",
      "[400]\ttraining's auc: 0.812233\tvalid_1's auc: 0.784964\n",
      "[500]\ttraining's auc: 0.814243\tvalid_1's auc: 0.788402\n",
      "[600]\ttraining's auc: 0.815855\tvalid_1's auc: 0.790774\n",
      "[700]\ttraining's auc: 0.816848\tvalid_1's auc: 0.792758\n",
      "[800]\ttraining's auc: 0.817511\tvalid_1's auc: 0.793966\n",
      "[900]\ttraining's auc: 0.81805\tvalid_1's auc: 0.795003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:01:38,254]\u001b[0m Trial 6 finished with value: 0.7959067743597293 and parameters: {'num_leaves': 82, 'learning_rate': 0.03767478682260475, 'feature_fraction': 0.10049293426844144, 'bagging_fraction': 0.6959437439954722, 'bagging_freq': 5, 'lambda_l1': 0.3669907219269512, 'lambda_l2': 0.06788781551513305}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.818462\tvalid_1's auc: 0.795907\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.818462\tvalid_1's auc: 0.795907\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.820266\tvalid_1's auc: 0.790176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:02:00,351]\u001b[0m Trial 7 finished with value: 0.8017195779364145 and parameters: {'num_leaves': 807, 'learning_rate': 0.002462862260324613, 'feature_fraction': 0.4961006816401461, 'bagging_fraction': 0.6880251127243885, 'bagging_freq': 3, 'lambda_l1': 1.8224399244948661, 'lambda_l2': 1.2832406194803993e-08}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.821079\tvalid_1's auc: 0.80172\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.822563\tvalid_1's auc: 0.799488\n",
      "[200]\ttraining's auc: 0.823875\tvalid_1's auc: 0.801194\n",
      "[300]\ttraining's auc: 0.825111\tvalid_1's auc: 0.802811\n",
      "[400]\ttraining's auc: 0.825981\tvalid_1's auc: 0.803538\n",
      "[500]\ttraining's auc: 0.826865\tvalid_1's auc: 0.804432\n",
      "[600]\ttraining's auc: 0.827734\tvalid_1's auc: 0.805106\n",
      "[700]\ttraining's auc: 0.828467\tvalid_1's auc: 0.805667\n",
      "[800]\ttraining's auc: 0.829264\tvalid_1's auc: 0.806468\n",
      "[900]\ttraining's auc: 0.829932\tvalid_1's auc: 0.807204\n",
      "[1000]\ttraining's auc: 0.830555\tvalid_1's auc: 0.807762\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.830555\tvalid_1's auc: 0.807762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:04:35,880]\u001b[0m Trial 8 finished with value: 0.8077623638504562 and parameters: {'num_leaves': 205, 'learning_rate': 0.0029947061452672698, 'feature_fraction': 0.6428455346840557, 'bagging_fraction': 0.444055066459103, 'bagging_freq': 9, 'lambda_l1': 0.02695373097037978, 'lambda_l2': 9.797432088338537e-07}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.805094\tvalid_1's auc: 0.769813\n",
      "[200]\ttraining's auc: 0.807091\tvalid_1's auc: 0.772416\n",
      "[300]\ttraining's auc: 0.809627\tvalid_1's auc: 0.775275\n",
      "[400]\ttraining's auc: 0.811503\tvalid_1's auc: 0.777441\n",
      "[500]\ttraining's auc: 0.81384\tvalid_1's auc: 0.78019\n",
      "[600]\ttraining's auc: 0.815447\tvalid_1's auc: 0.781593\n",
      "[700]\ttraining's auc: 0.817194\tvalid_1's auc: 0.783906\n",
      "[800]\ttraining's auc: 0.818931\tvalid_1's auc: 0.785953\n",
      "[900]\ttraining's auc: 0.820461\tvalid_1's auc: 0.78751\n",
      "[1000]\ttraining's auc: 0.821809\tvalid_1's auc: 0.789069\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.821809\tvalid_1's auc: 0.789069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:07:36,700]\u001b[0m Trial 9 finished with value: 0.7890693994701207 and parameters: {'num_leaves': 754, 'learning_rate': 0.003213466733385622, 'feature_fraction': 0.276964408495576, 'bagging_fraction': 0.4850939166357411, 'bagging_freq': 3, 'lambda_l1': 0.0030712342357467498, 'lambda_l2': 1.4454697589703585e-05}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.834493\tvalid_1's auc: 0.808321\n",
      "[200]\ttraining's auc: 0.841749\tvalid_1's auc: 0.811941\n",
      "[300]\ttraining's auc: 0.847552\tvalid_1's auc: 0.809908\n",
      "Early stopping, best iteration is:\n",
      "[202]\ttraining's auc: 0.84193\tvalid_1's auc: 0.812058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:08:38,315]\u001b[0m Trial 10 finished with value: 0.8120584339122756 and parameters: {'num_leaves': 995, 'learning_rate': 0.028296522977221628, 'feature_fraction': 0.9672666573215222, 'bagging_fraction': 0.15782276926645417, 'bagging_freq': 1, 'lambda_l1': 7.646321076850574, 'lambda_l2': 1.0080667969359109e-08}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.831861\tvalid_1's auc: 0.806571\n",
      "[200]\ttraining's auc: 0.837874\tvalid_1's auc: 0.809554\n",
      "[300]\ttraining's auc: 0.842536\tvalid_1's auc: 0.808818\n",
      "Early stopping, best iteration is:\n",
      "[212]\ttraining's auc: 0.838494\tvalid_1's auc: 0.80974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:09:38,232]\u001b[0m Trial 11 finished with value: 0.8097402119517221 and parameters: {'num_leaves': 962, 'learning_rate': 0.02617057768739891, 'feature_fraction': 0.9916065868376711, 'bagging_fraction': 0.10132628761025908, 'bagging_freq': 1, 'lambda_l1': 8.15021075963148, 'lambda_l2': 1.0113292122135543e-08}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.849261\tvalid_1's auc: 0.808227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:10:02,589]\u001b[0m Trial 12 finished with value: 0.8112879930821314 and parameters: {'num_leaves': 549, 'learning_rate': 0.0887151618473456, 'feature_fraction': 0.8430147995371227, 'bagging_fraction': 0.286471251451892, 'bagging_freq': 1, 'lambda_l1': 0.31660716905592595, 'lambda_l2': 8.821366307975741e-08}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's auc: 0.838134\tvalid_1's auc: 0.811288\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.838525\tvalid_1's auc: 0.809131\n",
      "[200]\ttraining's auc: 0.847542\tvalid_1's auc: 0.809998\n",
      "[300]\ttraining's auc: 0.856003\tvalid_1's auc: 0.811357\n",
      "[400]\ttraining's auc: 0.863139\tvalid_1's auc: 0.811198\n",
      "Early stopping, best iteration is:\n",
      "[305]\ttraining's auc: 0.85637\tvalid_1's auc: 0.811433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:11:23,660]\u001b[0m Trial 13 finished with value: 0.811432881954666 and parameters: {'num_leaves': 945, 'learning_rate': 0.022119724673206193, 'feature_fraction': 0.8134129089055528, 'bagging_fraction': 0.5714971022970474, 'bagging_freq': 3, 'lambda_l1': 0.00975392699693534, 'lambda_l2': 8.1985373228445e-05}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.828451\tvalid_1's auc: 0.805359\n",
      "[200]\ttraining's auc: 0.832255\tvalid_1's auc: 0.808418\n",
      "[300]\ttraining's auc: 0.835316\tvalid_1's auc: 0.808944\n",
      "[400]\ttraining's auc: 0.837837\tvalid_1's auc: 0.809331\n",
      "Early stopping, best iteration is:\n",
      "[350]\ttraining's auc: 0.836615\tvalid_1's auc: 0.80986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:12:39,769]\u001b[0m Trial 14 finished with value: 0.8098598027671475 and parameters: {'num_leaves': 537, 'learning_rate': 0.014446754242637663, 'feature_fraction': 0.985549459895514, 'bagging_fraction': 0.21469900832159589, 'bagging_freq': 7, 'lambda_l1': 9.994927869614749, 'lambda_l2': 3.414970795355619e-07}. Best is trial 4 with value: 0.8122010229614366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.853343\tvalid_1's auc: 0.81233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:13:18,129]\u001b[0m Trial 15 finished with value: 0.812626490285546 and parameters: {'num_leaves': 871, 'learning_rate': 0.06299407042788457, 'feature_fraction': 0.7307102617478988, 'bagging_fraction': 0.9824611836980233, 'bagging_freq': 2, 'lambda_l1': 0.09110421328063195, 'lambda_l2': 8.572194725132169e-08}. Best is trial 15 with value: 0.812626490285546.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.851095\tvalid_1's auc: 0.812626\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.853542\tvalid_1's auc: 0.811955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:13:48,825]\u001b[0m Trial 16 finished with value: 0.8127920775684427 and parameters: {'num_leaves': 578, 'learning_rate': 0.09481595207644586, 'feature_fraction': 0.7733922926780457, 'bagging_fraction': 0.9935899018501876, 'bagging_freq': 4, 'lambda_l1': 0.001848548575421097, 'lambda_l2': 3.5716280933244687e-05}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.849605\tvalid_1's auc: 0.812792\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.859936\tvalid_1's auc: 0.808609\n",
      "[200]\ttraining's auc: 0.877825\tvalid_1's auc: 0.808767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:14:35,564]\u001b[0m Trial 17 finished with value: 0.8096712172505153 and parameters: {'num_leaves': 843, 'learning_rate': 0.09005917366749196, 'feature_fraction': 0.7290916585673084, 'bagging_fraction': 0.9817452826968475, 'bagging_freq': 3, 'lambda_l1': 0.06482984227190676, 'lambda_l2': 2.8616618990143937e-05}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's auc: 0.871969\tvalid_1's auc: 0.809671\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.841305\tvalid_1's auc: 0.810382\n",
      "[200]\ttraining's auc: 0.851257\tvalid_1's auc: 0.80972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:15:13,941]\u001b[0m Trial 18 finished with value: 0.810782031939947 and parameters: {'num_leaves': 396, 'learning_rate': 0.05810242316641848, 'feature_fraction': 0.7279003990559625, 'bagging_fraction': 0.9767170762426503, 'bagging_freq': 4, 'lambda_l1': 0.001655588330314507, 'lambda_l2': 0.0003405907694517933}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's auc: 0.84506\tvalid_1's auc: 0.810782\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.846891\tvalid_1's auc: 0.811598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:15:48,375]\u001b[0m Trial 19 finished with value: 0.8119457425669709 and parameters: {'num_leaves': 608, 'learning_rate': 0.060167379884101174, 'feature_fraction': 0.7599925892971291, 'bagging_fraction': 0.9931309467075099, 'bagging_freq': 2, 'lambda_l1': 0.08100323470894666, 'lambda_l2': 6.505274418643534e-06}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.844372\tvalid_1's auc: 0.811946\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.845965\tvalid_1's auc: 0.810628\n",
      "[200]\ttraining's auc: 0.857408\tvalid_1's auc: 0.811168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:16:24,791]\u001b[0m Trial 20 finished with value: 0.8114811782455108 and parameters: {'num_leaves': 440, 'learning_rate': 0.07851979941799277, 'feature_fraction': 0.6574029296210883, 'bagging_fraction': 0.8944250102885665, 'bagging_freq': 4, 'lambda_l1': 0.00967333681208007, 'lambda_l2': 0.0003556680014863676}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's auc: 0.849696\tvalid_1's auc: 0.811481\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.840113\tvalid_1's auc: 0.810281\n",
      "[200]\ttraining's auc: 0.852093\tvalid_1's auc: 0.811028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:17:12,080]\u001b[0m Trial 21 finished with value: 0.8114282823079187 and parameters: {'num_leaves': 635, 'learning_rate': 0.036309713895405146, 'feature_fraction': 0.8551789777046852, 'bagging_fraction': 0.7902351368513536, 'bagging_freq': 6, 'lambda_l1': 0.0009521368000122069, 'lambda_l2': 1.1100142913268836e-07}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's auc: 0.847294\tvalid_1's auc: 0.811428\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85054\tvalid_1's auc: 0.812252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:17:46,048]\u001b[0m Trial 22 finished with value: 0.8125091992934943 and parameters: {'num_leaves': 867, 'learning_rate': 0.05616677205877089, 'feature_fraction': 0.8937045629803467, 'bagging_fraction': 0.5453124958109317, 'bagging_freq': 4, 'lambda_l1': 0.00010998149753008868, 'lambda_l2': 4.387329639748049e-06}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.845893\tvalid_1's auc: 0.812509\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.851007\tvalid_1's auc: 0.811543\n",
      "[200]\ttraining's auc: 0.867133\tvalid_1's auc: 0.811132\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's auc: 0.851177\tvalid_1's auc: 0.811649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:18:26,620]\u001b[0m Trial 23 finished with value: 0.811649065351781 and parameters: {'num_leaves': 875, 'learning_rate': 0.056585664253706654, 'feature_fraction': 0.7663193777792657, 'bagging_fraction': 0.9989844328681502, 'bagging_freq': 2, 'lambda_l1': 5.257248847264833e-05, 'lambda_l2': 5.211100930306731e-06}. Best is trial 16 with value: 0.8127920775684427.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.86251\tvalid_1's auc: 0.811842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:18:56,132]\u001b[0m Trial 24 finished with value: 0.8134820245805122 and parameters: {'num_leaves': 910, 'learning_rate': 0.08828627729795027, 'feature_fraction': 0.919042894244271, 'bagging_fraction': 0.9205359421651845, 'bagging_freq': 4, 'lambda_l1': 0.005273953273231081, 'lambda_l2': 4.381564320447384e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's auc: 0.847647\tvalid_1's auc: 0.813482\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85559\tvalid_1's auc: 0.810053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:19:28,338]\u001b[0m Trial 25 finished with value: 0.8102806704445099 and parameters: {'num_leaves': 681, 'learning_rate': 0.08588414201446758, 'feature_fraction': 0.9113483967573486, 'bagging_fraction': 0.9093958619212986, 'bagging_freq': 2, 'lambda_l1': 0.014459077042122445, 'lambda_l2': 8.319543345629393e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.851275\tvalid_1's auc: 0.810281\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.861343\tvalid_1's auc: 0.809453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:20:02,200]\u001b[0m Trial 26 finished with value: 0.8107613335295849 and parameters: {'num_leaves': 903, 'learning_rate': 0.08861052143040953, 'feature_fraction': 0.7987170766506547, 'bagging_fraction': 0.9337332898726506, 'bagging_freq': 6, 'lambda_l1': 0.0022223843527280356, 'lambda_l2': 3.75569653600202e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's auc: 0.855245\tvalid_1's auc: 0.810761\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.842735\tvalid_1's auc: 0.808639\n",
      "[200]\ttraining's auc: 0.856064\tvalid_1's auc: 0.810676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:20:58,656]\u001b[0m Trial 27 finished with value: 0.8110028149838092 and parameters: {'num_leaves': 779, 'learning_rate': 0.039948954390870525, 'feature_fraction': 0.6740813413053579, 'bagging_fraction': 0.9302208523811653, 'bagging_freq': 4, 'lambda_l1': 0.18292658049196905, 'lambda_l2': 0.0006924437308328184}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[189]\ttraining's auc: 0.854716\tvalid_1's auc: 0.811003\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.845484\tvalid_1's auc: 0.807215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:21:21,389]\u001b[0m Trial 28 finished with value: 0.8084017147483074 and parameters: {'num_leaves': 303, 'learning_rate': 0.0971375041574014, 'feature_fraction': 0.9286114453230631, 'bagging_fraction': 0.8063005009779297, 'bagging_freq': 2, 'lambda_l1': 0.03455210566336739, 'lambda_l2': 9.593261245546102e-06}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.835432\tvalid_1's auc: 0.808402\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.827927\tvalid_1's auc: 0.806468\n",
      "[200]\ttraining's auc: 0.830758\tvalid_1's auc: 0.808459\n",
      "[300]\ttraining's auc: 0.832359\tvalid_1's auc: 0.809262\n",
      "[400]\ttraining's auc: 0.833723\tvalid_1's auc: 0.809563\n",
      "[500]\ttraining's auc: 0.834809\tvalid_1's auc: 0.80963\n",
      "[600]\ttraining's auc: 0.835853\tvalid_1's auc: 0.809246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:22:35,052]\u001b[0m Trial 29 finished with value: 0.8097517110685899 and parameters: {'num_leaves': 24, 'learning_rate': 0.06560146838111905, 'feature_fraction': 0.912068482356148, 'bagging_fraction': 0.8360104181009584, 'bagging_freq': 6, 'lambda_l1': 0.7481806752656154, 'lambda_l2': 4.39585468546477}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's auc: 0.834918\tvalid_1's auc: 0.809752\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.846261\tvalid_1's auc: 0.810018\n",
      "[200]\ttraining's auc: 0.861775\tvalid_1's auc: 0.809273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:23:24,737]\u001b[0m Trial 30 finished with value: 0.8110787091551368 and parameters: {'num_leaves': 904, 'learning_rate': 0.043150413469875526, 'feature_fraction': 0.8041194385490648, 'bagging_fraction': 0.7209429256042569, 'bagging_freq': 4, 'lambda_l1': 0.006497878132537864, 'lambda_l2': 0.0010770336850376193}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's auc: 0.854383\tvalid_1's auc: 0.811079\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.854386\tvalid_1's auc: 0.810283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:24:00,445]\u001b[0m Trial 31 finished with value: 0.8109269208124816 and parameters: {'num_leaves': 862, 'learning_rate': 0.06531256414732613, 'feature_fraction': 0.8984287896756085, 'bagging_fraction': 0.7564686075376785, 'bagging_freq': 4, 'lambda_l1': 0.00035816525211337426, 'lambda_l2': 3.243272174906246e-06}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[86]\ttraining's auc: 0.850849\tvalid_1's auc: 0.810927\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.853929\tvalid_1's auc: 0.812976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:24:36,701]\u001b[0m Trial 32 finished with value: 0.8132865395937591 and parameters: {'num_leaves': 827, 'learning_rate': 0.06639973050396938, 'feature_fraction': 0.8961606571726262, 'bagging_fraction': 0.62772913846233, 'bagging_freq': 5, 'lambda_l1': 0.00016763643517009585, 'lambda_l2': 3.76769457665576e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.853593\tvalid_1's auc: 0.813287\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.856394\tvalid_1's auc: 0.810699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:25:07,781]\u001b[0m Trial 33 finished with value: 0.812150426847218 and parameters: {'num_leaves': 806, 'learning_rate': 0.07434094162988959, 'feature_fraction': 0.8458737996377396, 'bagging_fraction': 0.860632106699509, 'bagging_freq': 5, 'lambda_l1': 2.391200044262381e-05, 'lambda_l2': 0.00011419557301892207}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's auc: 0.846584\tvalid_1's auc: 0.81215\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.845549\tvalid_1's auc: 0.80974\n",
      "[200]\ttraining's auc: 0.860101\tvalid_1's auc: 0.810131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:25:50,008]\u001b[0m Trial 34 finished with value: 0.8112304974977921 and parameters: {'num_leaves': 712, 'learning_rate': 0.048738281579074026, 'feature_fraction': 0.9540498025132124, 'bagging_fraction': 0.6335711115680691, 'bagging_freq': 10, 'lambda_l1': 0.0009445057421524083, 'lambda_l2': 1.622251240376271e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's auc: 0.851429\tvalid_1's auc: 0.81123\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.866206\tvalid_1's auc: 0.811189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:26:24,366]\u001b[0m Trial 35 finished with value: 0.8124885008831322 and parameters: {'num_leaves': 942, 'learning_rate': 0.09738281371476014, 'feature_fraction': 0.8690119501883611, 'bagging_fraction': 0.862861961752002, 'bagging_freq': 5, 'lambda_l1': 0.004288735260395181, 'lambda_l2': 0.002148387570986201}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\ttraining's auc: 0.860102\tvalid_1's auc: 0.812489\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.846496\tvalid_1's auc: 0.811145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:26:54,165]\u001b[0m Trial 36 finished with value: 0.8114811782455108 and parameters: {'num_leaves': 470, 'learning_rate': 0.06976894881261714, 'feature_fraction': 0.939877549707854, 'bagging_fraction': 0.9371550469289319, 'bagging_freq': 8, 'lambda_l1': 0.0002586304120008184, 'lambda_l2': 0.0001386940623638809}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's auc: 0.843321\tvalid_1's auc: 0.811481\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.839155\tvalid_1's auc: 0.810478\n",
      "[200]\ttraining's auc: 0.850058\tvalid_1's auc: 0.812705\n",
      "[300]\ttraining's auc: 0.857922\tvalid_1's auc: 0.812725\n",
      "[400]\ttraining's auc: 0.864351\tvalid_1's auc: 0.812712\n",
      "Early stopping, best iteration is:\n",
      "[372]\ttraining's auc: 0.862628\tvalid_1's auc: 0.813149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:28:17,750]\u001b[0m Trial 37 finished with value: 0.8131485501913452 and parameters: {'num_leaves': 658, 'learning_rate': 0.034878426017843724, 'feature_fraction': 0.7097896929774541, 'bagging_fraction': 0.7536337879439974, 'bagging_freq': 5, 'lambda_l1': 0.023690822824041485, 'lambda_l2': 0.00792622797954652}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.840819\tvalid_1's auc: 0.809352\n",
      "[200]\ttraining's auc: 0.853567\tvalid_1's auc: 0.811359\n",
      "[300]\ttraining's auc: 0.862236\tvalid_1's auc: 0.811357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:29:12,124]\u001b[0m Trial 38 finished with value: 0.8115271747129822 and parameters: {'num_leaves': 665, 'learning_rate': 0.04580424711634552, 'feature_fraction': 0.6098188631857525, 'bagging_fraction': 0.729946239570396, 'bagging_freq': 5, 'lambda_l1': 0.0007274201503300868, 'lambda_l2': 0.014407303702079557}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[204]\ttraining's auc: 0.853926\tvalid_1's auc: 0.811527\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.8378\tvalid_1's auc: 0.810405\n",
      "[200]\ttraining's auc: 0.847882\tvalid_1's auc: 0.81253\n",
      "[300]\ttraining's auc: 0.855036\tvalid_1's auc: 0.812153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:30:10,114]\u001b[0m Trial 39 finished with value: 0.8129254673241096 and parameters: {'num_leaves': 604, 'learning_rate': 0.03365366986761467, 'feature_fraction': 0.6922073241038355, 'bagging_fraction': 0.6451247913599678, 'bagging_freq': 6, 'lambda_l1': 1.5894381829060876e-05, 'lambda_l2': 0.2479096325075768}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's auc: 0.849663\tvalid_1's auc: 0.812925\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.831883\tvalid_1's auc: 0.804297\n",
      "[200]\ttraining's auc: 0.839696\tvalid_1's auc: 0.808901\n",
      "[300]\ttraining's auc: 0.84578\tvalid_1's auc: 0.811815\n",
      "[400]\ttraining's auc: 0.851156\tvalid_1's auc: 0.811684\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's auc: 0.846407\tvalid_1's auc: 0.811957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:31:28,942]\u001b[0m Trial 40 finished with value: 0.8119572416838388 and parameters: {'num_leaves': 738, 'learning_rate': 0.018771200834084824, 'feature_fraction': 0.5367714090991335, 'bagging_fraction': 0.6279466435830345, 'bagging_freq': 7, 'lambda_l1': 6.813432778259579e-06, 'lambda_l2': 0.4392567065611612}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.837071\tvalid_1's auc: 0.808301\n",
      "[200]\ttraining's auc: 0.846575\tvalid_1's auc: 0.810518\n",
      "[300]\ttraining's auc: 0.853242\tvalid_1's auc: 0.810651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:32:31,040]\u001b[0m Trial 41 finished with value: 0.8111477038563437 and parameters: {'num_leaves': 549, 'learning_rate': 0.03401532591543454, 'feature_fraction': 0.6968634279280952, 'bagging_fraction': 0.7647596917904285, 'bagging_freq': 6, 'lambda_l1': 0.0004273717989351749, 'lambda_l2': 1.0438024652270663}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[258]\ttraining's auc: 0.850916\tvalid_1's auc: 0.811148\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.838121\tvalid_1's auc: 0.810725\n",
      "[200]\ttraining's auc: 0.847988\tvalid_1's auc: 0.812026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:33:16,875]\u001b[0m Trial 42 finished with value: 0.8125758941713277 and parameters: {'num_leaves': 592, 'learning_rate': 0.03305750302999574, 'feature_fraction': 0.7899887723507408, 'bagging_fraction': 0.6410117790796617, 'bagging_freq': 5, 'lambda_l1': 0.00013960428952389776, 'lambda_l2': 0.19442968039621183}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's auc: 0.843361\tvalid_1's auc: 0.812576\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.840409\tvalid_1's auc: 0.810209\n",
      "[200]\ttraining's auc: 0.851169\tvalid_1's auc: 0.812019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:34:02,697]\u001b[0m Trial 43 finished with value: 0.8124931005298792 and parameters: {'num_leaves': 483, 'learning_rate': 0.04926676692669532, 'feature_fraction': 0.6257270098490151, 'bagging_fraction': 0.8288669570083136, 'bagging_freq': 8, 'lambda_l1': 0.0022565213182577266, 'lambda_l2': 0.00821499443284301}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[173]\ttraining's auc: 0.848485\tvalid_1's auc: 0.812493\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.849992\tvalid_1's auc: 0.809257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:34:37,439]\u001b[0m Trial 44 finished with value: 0.8097218133647335 and parameters: {'num_leaves': 650, 'learning_rate': 0.07438480100275105, 'feature_fraction': 0.687282691073355, 'bagging_fraction': 0.6792395956836863, 'bagging_freq': 5, 'lambda_l1': 0.020441493267666193, 'lambda_l2': 0.041342748682619654}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's auc: 0.848906\tvalid_1's auc: 0.809722\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.835385\tvalid_1's auc: 0.807353\n",
      "[200]\ttraining's auc: 0.844279\tvalid_1's auc: 0.809906\n",
      "[300]\ttraining's auc: 0.850264\tvalid_1's auc: 0.810138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:35:35,483]\u001b[0m Trial 45 finished with value: 0.8103611642625846 and parameters: {'num_leaves': 367, 'learning_rate': 0.04432422736495813, 'feature_fraction': 0.5738804035784644, 'bagging_fraction': 0.7767741859512245, 'bagging_freq': 6, 'lambda_l1': 5.112307534279379e-06, 'lambda_l2': 0.0033452651011565923}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[270]\ttraining's auc: 0.848597\tvalid_1's auc: 0.810361\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.836254\tvalid_1's auc: 0.809156\n",
      "[200]\ttraining's auc: 0.844674\tvalid_1's auc: 0.811049\n",
      "[300]\ttraining's auc: 0.850983\tvalid_1's auc: 0.810913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:36:34,488]\u001b[0m Trial 46 finished with value: 0.8112879930821313 and parameters: {'num_leaves': 586, 'learning_rate': 0.028448302908117267, 'feature_fraction': 0.8678714302953325, 'bagging_fraction': 0.8985412516276299, 'bagging_freq': 7, 'lambda_l1': 0.004931789141937299, 'lambda_l2': 7.297955402418746}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's auc: 0.845206\tvalid_1's auc: 0.811288\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.831466\tvalid_1's auc: 0.805801\n",
      "[200]\ttraining's auc: 0.834033\tvalid_1's auc: 0.807491\n",
      "[300]\ttraining's auc: 0.836623\tvalid_1's auc: 0.808974\n",
      "[400]\ttraining's auc: 0.839101\tvalid_1's auc: 0.809782\n",
      "[500]\ttraining's auc: 0.84174\tvalid_1's auc: 0.810955\n",
      "[600]\ttraining's auc: 0.844319\tvalid_1's auc: 0.811649\n",
      "[700]\ttraining's auc: 0.847051\tvalid_1's auc: 0.812061\n",
      "[800]\ttraining's auc: 0.849497\tvalid_1's auc: 0.812392\n",
      "[900]\ttraining's auc: 0.851973\tvalid_1's auc: 0.812475\n",
      "[1000]\ttraining's auc: 0.854397\tvalid_1's auc: 0.812677\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.854397\tvalid_1's auc: 0.812677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:39:59,209]\u001b[0m Trial 47 finished with value: 0.8126770863997644 and parameters: {'num_leaves': 797, 'learning_rate': 0.006835393104215111, 'feature_fraction': 0.8119876164588208, 'bagging_fraction': 0.8205078739962519, 'bagging_freq': 3, 'lambda_l1': 1.8543102817795454e-07, 'lambda_l2': 0.0012875409602434694}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.833013\tvalid_1's auc: 0.805939\n",
      "[200]\ttraining's auc: 0.837423\tvalid_1's auc: 0.808606\n",
      "[300]\ttraining's auc: 0.841915\tvalid_1's auc: 0.810814\n",
      "[400]\ttraining's auc: 0.846411\tvalid_1's auc: 0.811525\n",
      "[500]\ttraining's auc: 0.850635\tvalid_1's auc: 0.812058\n",
      "[600]\ttraining's auc: 0.854492\tvalid_1's auc: 0.812284\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttraining's auc: 0.854276\tvalid_1's auc: 0.812374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:42:11,688]\u001b[0m Trial 48 finished with value: 0.812373509714454 and parameters: {'num_leaves': 710, 'learning_rate': 0.012745009245476707, 'feature_fraction': 0.7604788166882899, 'bagging_fraction': 0.8734402647780386, 'bagging_freq': 5, 'lambda_l1': 0.00014625217238643532, 'lambda_l2': 4.4352446638560736e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.834178\tvalid_1's auc: 0.808425\n",
      "[200]\ttraining's auc: 0.841695\tvalid_1's auc: 0.809867\n",
      "[300]\ttraining's auc: 0.848759\tvalid_1's auc: 0.810152\n",
      "Early stopping, best iteration is:\n",
      "[294]\ttraining's auc: 0.84845\tvalid_1's auc: 0.810437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:43:21,071]\u001b[0m Trial 49 finished with value: 0.8104370584339123 and parameters: {'num_leaves': 532, 'learning_rate': 0.02317174055334265, 'feature_fraction': 0.990284790168256, 'bagging_fraction': 0.7490210778137779, 'bagging_freq': 4, 'lambda_l1': 4.466118089780291e-05, 'lambda_l2': 0.00018573038037464863}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.830714\tvalid_1's auc: 0.80727\n",
      "[200]\ttraining's auc: 0.836165\tvalid_1's auc: 0.809586\n",
      "[300]\ttraining's auc: 0.839741\tvalid_1's auc: 0.809754\n",
      "[400]\ttraining's auc: 0.842272\tvalid_1's auc: 0.809657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:44:28,087]\u001b[0m Trial 50 finished with value: 0.8100621872240212 and parameters: {'num_leaves': 173, 'learning_rate': 0.03421876766823819, 'feature_fraction': 0.7105981874821898, 'bagging_fraction': 0.6972695710008457, 'bagging_freq': 6, 'lambda_l1': 0.022626666094273596, 'lambda_l2': 0.005387070028398797}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[371]\ttraining's auc: 0.841466\tvalid_1's auc: 0.810062\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.831922\tvalid_1's auc: 0.80638\n",
      "[200]\ttraining's auc: 0.834645\tvalid_1's auc: 0.807183\n",
      "[300]\ttraining's auc: 0.83736\tvalid_1's auc: 0.808471\n",
      "[400]\ttraining's auc: 0.840216\tvalid_1's auc: 0.809582\n",
      "[500]\ttraining's auc: 0.843127\tvalid_1's auc: 0.810573\n",
      "[600]\ttraining's auc: 0.846265\tvalid_1's auc: 0.810975\n",
      "[700]\ttraining's auc: 0.849168\tvalid_1's auc: 0.811203\n",
      "[800]\ttraining's auc: 0.851868\tvalid_1's auc: 0.81132\n",
      "[900]\ttraining's auc: 0.854622\tvalid_1's auc: 0.811518\n",
      "[1000]\ttraining's auc: 0.857159\tvalid_1's auc: 0.81196\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.857159\tvalid_1's auc: 0.81196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:47:50,042]\u001b[0m Trial 51 finished with value: 0.8119595415072123 and parameters: {'num_leaves': 803, 'learning_rate': 0.007401109081392571, 'feature_fraction': 0.8393117314036176, 'bagging_fraction': 0.825610118034985, 'bagging_freq': 3, 'lambda_l1': 1.6023388426125533e-07, 'lambda_l2': 0.0010853624954608844}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.830542\tvalid_1's auc: 0.804294\n",
      "[200]\ttraining's auc: 0.83249\tvalid_1's auc: 0.805743\n",
      "[300]\ttraining's auc: 0.834482\tvalid_1's auc: 0.807098\n",
      "[400]\ttraining's auc: 0.836308\tvalid_1's auc: 0.808059\n",
      "[500]\ttraining's auc: 0.83819\tvalid_1's auc: 0.808859\n",
      "[600]\ttraining's auc: 0.840075\tvalid_1's auc: 0.809519\n",
      "[700]\ttraining's auc: 0.842058\tvalid_1's auc: 0.810101\n",
      "[800]\ttraining's auc: 0.843952\tvalid_1's auc: 0.810711\n",
      "[900]\ttraining's auc: 0.845874\tvalid_1's auc: 0.811005\n",
      "[1000]\ttraining's auc: 0.847822\tvalid_1's auc: 0.811355\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.847822\tvalid_1's auc: 0.811355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:51:22,065]\u001b[0m Trial 52 finished with value: 0.8113546879599647 and parameters: {'num_leaves': 762, 'learning_rate': 0.00516831276310031, 'feature_fraction': 0.8231774151395358, 'bagging_fraction': 0.9530272828999568, 'bagging_freq': 3, 'lambda_l1': 3.899326602389085e-08, 'lambda_l2': 0.00021538522128244656}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.853109\tvalid_1's auc: 0.812564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:52:00,817]\u001b[0m Trial 53 finished with value: 0.8128541727995291 and parameters: {'num_leaves': 999, 'learning_rate': 0.05677413524571376, 'feature_fraction': 0.74988608709019, 'bagging_fraction': 0.8115196365903293, 'bagging_freq': 4, 'lambda_l1': 1.2614597951021287e-06, 'lambda_l2': 0.0005837192546166238}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[89]\ttraining's auc: 0.850638\tvalid_1's auc: 0.812854\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.859186\tvalid_1's auc: 0.809703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:52:34,130]\u001b[0m Trial 54 finished with value: 0.8102553723874006 and parameters: {'num_leaves': 941, 'learning_rate': 0.07940756787634415, 'feature_fraction': 0.7437261226051711, 'bagging_fraction': 0.7929301988855878, 'bagging_freq': 4, 'lambda_l1': 1.7388438498200657e-06, 'lambda_l2': 0.0004375803200952037}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.849986\tvalid_1's auc: 0.810255\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.850085\tvalid_1's auc: 0.810863\n",
      "[200]\ttraining's auc: 0.866646\tvalid_1's auc: 0.810354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:53:26,541]\u001b[0m Trial 55 finished with value: 0.8112120989108037 and parameters: {'num_leaves': 963, 'learning_rate': 0.0521826910902473, 'feature_fraction': 0.6618890830519388, 'bagging_fraction': 0.8751184746969698, 'bagging_freq': 5, 'lambda_l1': 1.9599396474385122e-05, 'lambda_l2': 2.1552258296054456e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's auc: 0.861497\tvalid_1's auc: 0.811212\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.854878\tvalid_1's auc: 0.811787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:54:03,894]\u001b[0m Trial 56 finished with value: 0.8125643950544599 and parameters: {'num_leaves': 910, 'learning_rate': 0.06537435822241588, 'feature_fraction': 0.7706209280942548, 'bagging_fraction': 0.9612479172490325, 'bagging_freq': 4, 'lambda_l1': 0.004123956900921172, 'lambda_l2': 7.294904284729923e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's auc: 0.851734\tvalid_1's auc: 0.812564\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.852134\tvalid_1's auc: 0.812139\n",
      "[200]\ttraining's auc: 0.869807\tvalid_1's auc: 0.811773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:54:46,945]\u001b[0m Trial 57 finished with value: 0.8125551957609656 and parameters: {'num_leaves': 968, 'learning_rate': 0.05569331190865971, 'feature_fraction': 0.7284639835478369, 'bagging_fraction': 0.8919288759624484, 'bagging_freq': 5, 'lambda_l1': 0.0012518334838889553, 'lambda_l2': 4.926110366344352e-05}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's auc: 0.854854\tvalid_1's auc: 0.812555\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.861114\tvalid_1's auc: 0.809954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:55:21,181]\u001b[0m Trial 58 finished with value: 0.8131485501913454 and parameters: {'num_leaves': 993, 'learning_rate': 0.07945863573713279, 'feature_fraction': 0.8830352036822614, 'bagging_fraction': 0.5979807155130894, 'bagging_freq': 4, 'lambda_l1': 0.009106252137354703, 'lambda_l2': 0.0005132104918565297}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.854204\tvalid_1's auc: 0.813149\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.847177\tvalid_1's auc: 0.810175\n",
      "[200]\ttraining's auc: 0.863143\tvalid_1's auc: 0.81003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:56:04,849]\u001b[0m Trial 59 finished with value: 0.8108533264645275 and parameters: {'num_leaves': 993, 'learning_rate': 0.041073149495969005, 'feature_fraction': 0.9543662014436477, 'bagging_fraction': 0.5922917296032384, 'bagging_freq': 3, 'lambda_l1': 0.05939890936644508, 'lambda_l2': 0.002816845421525193}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's auc: 0.849457\tvalid_1's auc: 0.810853\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.860318\tvalid_1's auc: 0.811173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:56:35,368]\u001b[0m Trial 60 finished with value: 0.8131439505445983 and parameters: {'num_leaves': 998, 'learning_rate': 0.07847322828965776, 'feature_fraction': 0.8920864720310072, 'bagging_fraction': 0.534642139970882, 'bagging_freq': 6, 'lambda_l1': 0.04491611172073994, 'lambda_l2': 0.018558317333886135}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.842723\tvalid_1's auc: 0.813144\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.860308\tvalid_1's auc: 0.809043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:57:08,160]\u001b[0m Trial 61 finished with value: 0.8106647409478953 and parameters: {'num_leaves': 994, 'learning_rate': 0.07889695044964541, 'feature_fraction': 0.8675998088727328, 'bagging_fraction': 0.516194958164118, 'bagging_freq': 6, 'lambda_l1': 0.015325658202208773, 'lambda_l2': 0.0004919148378066757}. Best is trial 24 with value: 0.8134820245805122.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's auc: 0.850991\tvalid_1's auc: 0.810665\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.852612\tvalid_1's auc: 0.812562\n",
      "[200]\ttraining's auc: 0.869991\tvalid_1's auc: 0.811189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:57:50,157]\u001b[0m Trial 62 finished with value: 0.8139074919046217 and parameters: {'num_leaves': 914, 'learning_rate': 0.05856625324959715, 'feature_fraction': 0.895552049755324, 'bagging_fraction': 0.6076848986308129, 'bagging_freq': 7, 'lambda_l1': 0.008228463273101132, 'lambda_l2': 0.012077846321216792}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[117]\ttraining's auc: 0.855893\tvalid_1's auc: 0.813907\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.861618\tvalid_1's auc: 0.808503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:58:16,307]\u001b[0m Trial 63 finished with value: 0.8110534110980278 and parameters: {'num_leaves': 841, 'learning_rate': 0.09925755667946799, 'feature_fraction': 0.8965122142342117, 'bagging_fraction': 0.5866984959208124, 'bagging_freq': 8, 'lambda_l1': 0.006336471986325564, 'lambda_l2': 0.018348394183215037}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.843896\tvalid_1's auc: 0.811053\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85601\tvalid_1's auc: 0.807638\n",
      "[200]\ttraining's auc: 0.875186\tvalid_1's auc: 0.805821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:58:55,935]\u001b[0m Trial 64 finished with value: 0.8094642331468943 and parameters: {'num_leaves': 916, 'learning_rate': 0.0694455162964047, 'feature_fraction': 0.9630441497962567, 'bagging_fraction': 0.5427877347050609, 'bagging_freq': 7, 'lambda_l1': 0.04316299231947613, 'lambda_l2': 0.06593188923857922}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's auc: 0.859711\tvalid_1's auc: 0.809464\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.856752\tvalid_1's auc: 0.80917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 08:59:26,698]\u001b[0m Trial 65 finished with value: 0.8114719789520166 and parameters: {'num_leaves': 837, 'learning_rate': 0.08187616006889804, 'feature_fraction': 0.8747322698641393, 'bagging_fraction': 0.47178864169185486, 'bagging_freq': 7, 'lambda_l1': 0.011035826582540032, 'lambda_l2': 0.031999270886099176}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's auc: 0.848257\tvalid_1's auc: 0.811472\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.852666\tvalid_1's auc: 0.810909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:00:00,635]\u001b[0m Trial 66 finished with value: 0.8113937849573153 and parameters: {'num_leaves': 889, 'learning_rate': 0.06074408464618363, 'feature_fraction': 0.9228146223667848, 'bagging_fraction': 0.6632331522376091, 'bagging_freq': 6, 'lambda_l1': 0.03202376672809282, 'lambda_l2': 0.00793508825881421}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's auc: 0.847473\tvalid_1's auc: 0.811394\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.848989\tvalid_1's auc: 0.813353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:00:39,508]\u001b[0m Trial 67 finished with value: 0.813826998086547 and parameters: {'num_leaves': 926, 'learning_rate': 0.05026408528445307, 'feature_fraction': 0.8375159288364995, 'bagging_fraction': 0.5967089866891111, 'bagging_freq': 6, 'lambda_l1': 0.124670401306131, 'lambda_l2': 0.13910136906347714}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's auc: 0.848035\tvalid_1's auc: 0.813827\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.848982\tvalid_1's auc: 0.811546\n",
      "[200]\ttraining's auc: 0.864397\tvalid_1's auc: 0.811056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:01:29,320]\u001b[0m Trial 68 finished with value: 0.8124333051221666 and parameters: {'num_leaves': 919, 'learning_rate': 0.049409451360680826, 'feature_fraction': 0.9302997363684573, 'bagging_fraction': 0.6047337497719444, 'bagging_freq': 7, 'lambda_l1': 0.13513605513382348, 'lambda_l2': 0.004026936154133971}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's auc: 0.859167\tvalid_1's auc: 0.812433\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.853894\tvalid_1's auc: 0.809501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:02:05,961]\u001b[0m Trial 69 finished with value: 0.8099402965852223 and parameters: {'num_leaves': 831, 'learning_rate': 0.07224203226207612, 'feature_fraction': 0.9956961749687533, 'bagging_fraction': 0.5533502123920813, 'bagging_freq': 8, 'lambda_l1': 0.22516825077671887, 'lambda_l2': 0.10362048531116401}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.853299\tvalid_1's auc: 0.80994\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.844793\tvalid_1's auc: 0.810566\n",
      "[200]\ttraining's auc: 0.858828\tvalid_1's auc: 0.81038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:02:55,158]\u001b[0m Trial 70 finished with value: 0.8115777708272005 and parameters: {'num_leaves': 936, 'learning_rate': 0.03918466313107381, 'feature_fraction': 0.8319794951076709, 'bagging_fraction': 0.5104482719364949, 'bagging_freq': 5, 'lambda_l1': 0.09917229575401842, 'lambda_l2': 0.022303117058671156}. Best is trial 62 with value: 0.8139074919046217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's auc: 0.852424\tvalid_1's auc: 0.811578\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.863128\tvalid_1's auc: 0.813974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:03:29,799]\u001b[0m Trial 71 finished with value: 0.8147607263762142 and parameters: {'num_leaves': 968, 'learning_rate': 0.08763502043577061, 'feature_fraction': 0.9019125656696226, 'bagging_fraction': 0.6170675368292509, 'bagging_freq': 6, 'lambda_l1': 0.05275656010203414, 'lambda_l2': 0.01144787273227786}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's auc: 0.857887\tvalid_1's auc: 0.814761\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.862239\tvalid_1's auc: 0.807509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:03:57,367]\u001b[0m Trial 72 finished with value: 0.8094757322637621 and parameters: {'num_leaves': 964, 'learning_rate': 0.08821556513732041, 'feature_fraction': 0.8848317129423998, 'bagging_fraction': 0.6120695319839116, 'bagging_freq': 6, 'lambda_l1': 0.07564163738012054, 'lambda_l2': 0.01123635339507627}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's auc: 0.843777\tvalid_1's auc: 0.809476\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.853148\tvalid_1's auc: 0.8096\n",
      "[200]\ttraining's auc: 0.87028\tvalid_1's auc: 0.807746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:04:38,822]\u001b[0m Trial 73 finished with value: 0.8106371430674124 and parameters: {'num_leaves': 886, 'learning_rate': 0.06206631086241098, 'feature_fraction': 0.9119954475664043, 'bagging_fraction': 0.5807160080925496, 'bagging_freq': 7, 'lambda_l1': 0.039049924427517015, 'lambda_l2': 0.03153683309268804}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's auc: 0.856694\tvalid_1's auc: 0.810637\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.862513\tvalid_1's auc: 0.812969\n",
      "[200]\ttraining's auc: 0.883338\tvalid_1's auc: 0.81161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:05:20,460]\u001b[0m Trial 74 finished with value: 0.8133371357079776 and parameters: {'num_leaves': 972, 'learning_rate': 0.08484967954393172, 'feature_fraction': 0.8517883415471328, 'bagging_fraction': 0.6528266751862122, 'bagging_freq': 5, 'lambda_l1': 0.009117593107068044, 'lambda_l2': 0.008191122598567007}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.868001\tvalid_1's auc: 0.813337\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.86351\tvalid_1's auc: 0.810773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:05:47,279]\u001b[0m Trial 75 finished with value: 0.8115731711804532 and parameters: {'num_leaves': 864, 'learning_rate': 0.09891780205011087, 'feature_fraction': 0.8486286742696364, 'bagging_fraction': 0.6611533065103118, 'bagging_freq': 5, 'lambda_l1': 0.010413352644369647, 'lambda_l2': 0.006377512384623025}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.844651\tvalid_1's auc: 0.811573\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.862563\tvalid_1's auc: 0.808432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:06:18,313]\u001b[0m Trial 76 finished with value: 0.8099471960553429 and parameters: {'num_leaves': 934, 'learning_rate': 0.08517072127299215, 'feature_fraction': 0.9454497639025792, 'bagging_fraction': 0.6121370337477468, 'bagging_freq': 5, 'lambda_l1': 0.0024221666841334915, 'lambda_l2': 0.0025616015823152535}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's auc: 0.850501\tvalid_1's auc: 0.809947\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.849254\tvalid_1's auc: 0.812291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:06:55,821]\u001b[0m Trial 77 finished with value: 0.8124126067118045 and parameters: {'num_leaves': 891, 'learning_rate': 0.052216134045329696, 'feature_fraction': 0.7990018802273104, 'bagging_fraction': 0.7085982412641214, 'bagging_freq': 4, 'lambda_l1': 0.017849633963317416, 'lambda_l2': 0.011398472857481818}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.8473\tvalid_1's auc: 0.812413\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85709\tvalid_1's auc: 0.810598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:07:30,033]\u001b[0m Trial 78 finished with value: 0.812272317486017 and parameters: {'num_leaves': 966, 'learning_rate': 0.06756359234000273, 'feature_fraction': 0.9733037339999927, 'bagging_fraction': 0.5667436215340285, 'bagging_freq': 5, 'lambda_l1': 0.007660810183818202, 'lambda_l2': 0.0013322091303120199}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.851021\tvalid_1's auc: 0.812272\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.845893\tvalid_1's auc: 0.811235\n",
      "[200]\ttraining's auc: 0.861145\tvalid_1's auc: 0.810465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:08:17,389]\u001b[0m Trial 79 finished with value: 0.811934243450103 and parameters: {'num_leaves': 824, 'learning_rate': 0.04529360499158205, 'feature_fraction': 0.8564593792884614, 'bagging_fraction': 0.6896876328497437, 'bagging_freq': 6, 'lambda_l1': 0.39402896197512965, 'lambda_l2': 0.000237509427382134}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's auc: 0.854001\tvalid_1's auc: 0.811934\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.852587\tvalid_1's auc: 0.810603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:08:53,721]\u001b[0m Trial 80 finished with value: 0.8109821165734471 and parameters: {'num_leaves': 857, 'learning_rate': 0.059806709814959, 'feature_fraction': 0.9331999849982173, 'bagging_fraction': 0.7327140783429528, 'bagging_freq': 4, 'lambda_l1': 0.002826291769493588, 'lambda_l2': 0.00452856337427034}. Best is trial 71 with value: 0.8147607263762142.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.849528\tvalid_1's auc: 0.810982\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.858893\tvalid_1's auc: 0.812935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:09:26,609]\u001b[0m Trial 81 finished with value: 0.8148113224904328 and parameters: {'num_leaves': 974, 'learning_rate': 0.07496665344106089, 'feature_fraction': 0.9011253799278782, 'bagging_fraction': 0.6220601458405756, 'bagging_freq': 6, 'lambda_l1': 0.05582306915107806, 'lambda_l2': 0.06944996697439584}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.849665\tvalid_1's auc: 0.814811\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.863313\tvalid_1's auc: 0.808301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:09:55,276]\u001b[0m Trial 82 finished with value: 0.8093170444509861 and parameters: {'num_leaves': 971, 'learning_rate': 0.08892390047836912, 'feature_fraction': 0.9044589222017267, 'bagging_fraction': 0.6544190914473066, 'bagging_freq': 6, 'lambda_l1': 0.09861421367075907, 'lambda_l2': 0.04971462828905216}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's auc: 0.845488\tvalid_1's auc: 0.809317\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.856787\tvalid_1's auc: 0.812815\n",
      "[200]\ttraining's auc: 0.874929\tvalid_1's auc: 0.8102\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's auc: 0.856918\tvalid_1's auc: 0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:10:33,610]\u001b[0m Trial 83 finished with value: 0.8129001692670003 and parameters: {'num_leaves': 924, 'learning_rate': 0.06982641598358423, 'feature_fraction': 0.8351484451069361, 'bagging_fraction': 0.6277859885099975, 'bagging_freq': 5, 'lambda_l1': 0.030726901198714646, 'lambda_l2': 0.10545409261196531}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.859374\tvalid_1's auc: 0.811001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:11:06,191]\u001b[0m Trial 84 finished with value: 0.8130289593759199 and parameters: {'num_leaves': 946, 'learning_rate': 0.0766382779073131, 'feature_fraction': 0.8809757197969864, 'bagging_fraction': 0.6703664008492839, 'bagging_freq': 6, 'lambda_l1': 0.01340770284734141, 'lambda_l2': 0.02770566623114721}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's auc: 0.849658\tvalid_1's auc: 0.813029\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.847595\tvalid_1's auc: 0.811258\n",
      "[200]\ttraining's auc: 0.863227\tvalid_1's auc: 0.810849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:11:49,945]\u001b[0m Trial 85 finished with value: 0.8116996614659994 and parameters: {'num_leaves': 778, 'learning_rate': 0.054230106248333756, 'feature_fraction': 0.822374538889744, 'bagging_fraction': 0.5672960060254657, 'bagging_freq': 5, 'lambda_l1': 0.006110892122571244, 'lambda_l2': 0.011194779491882752}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[141]\ttraining's auc: 0.855\tvalid_1's auc: 0.8117\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.859616\tvalid_1's auc: 0.806907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:12:23,102]\u001b[0m Trial 86 finished with value: 0.8093814395054459 and parameters: {'num_leaves': 902, 'learning_rate': 0.08829790010635857, 'feature_fraction': 0.7820997481328502, 'bagging_fraction': 0.610936066721611, 'bagging_freq': 7, 'lambda_l1': 0.02333540467433054, 'lambda_l2': 0.05431036954146245}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.853314\tvalid_1's auc: 0.809381\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.855158\tvalid_1's auc: 0.812436\n",
      "[200]\ttraining's auc: 0.873222\tvalid_1's auc: 0.808462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:13:02,839]\u001b[0m Trial 87 finished with value: 0.8127552803944657 and parameters: {'num_leaves': 951, 'learning_rate': 0.061866558478595915, 'feature_fraction': 0.912388248621547, 'bagging_fraction': 0.7003050596413571, 'bagging_freq': 7, 'lambda_l1': 0.0036253517739877235, 'lambda_l2': 1.241389598145953e-05}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's auc: 0.855586\tvalid_1's auc: 0.812755\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.859975\tvalid_1's auc: 0.81072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:13:35,238]\u001b[0m Trial 88 finished with value: 0.8112074992640566 and parameters: {'num_leaves': 978, 'learning_rate': 0.07438462634938021, 'feature_fraction': 0.9702195167216268, 'bagging_fraction': 0.6364878835518716, 'bagging_freq': 6, 'lambda_l1': 0.15127217274424076, 'lambda_l2': 0.0018770985291326442}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's auc: 0.848606\tvalid_1's auc: 0.811207\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.847922\tvalid_1's auc: 0.811909\n",
      "[200]\ttraining's auc: 0.863877\tvalid_1's auc: 0.811946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:14:20,856]\u001b[0m Trial 89 finished with value: 0.812568994701207 and parameters: {'num_leaves': 880, 'learning_rate': 0.04694358200017941, 'feature_fraction': 0.8570419687760843, 'bagging_fraction': 0.6525207120888163, 'bagging_freq': 5, 'lambda_l1': 0.0013758273324007404, 'lambda_l2': 0.007708713265252215}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[139]\ttraining's auc: 0.85499\tvalid_1's auc: 0.812569\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.856686\tvalid_1's auc: 0.809991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:14:52,574]\u001b[0m Trial 90 finished with value: 0.8105014534883721 and parameters: {'num_leaves': 926, 'learning_rate': 0.06728669602829045, 'feature_fraction': 0.9454875383494383, 'bagging_fraction': 0.6783551553168806, 'bagging_freq': 4, 'lambda_l1': 0.060299590219793024, 'lambda_l2': 0.01852158358961704}. Best is trial 81 with value: 0.8148113224904328.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's auc: 0.844837\tvalid_1's auc: 0.810501\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.862076\tvalid_1's auc: 0.815763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:15:26,431]\u001b[0m Trial 91 finished with value: 0.8176815020606417 and parameters: {'num_leaves': 998, 'learning_rate': 0.0820671592743757, 'feature_fraction': 0.8930950799841506, 'bagging_fraction': 0.5944132149154198, 'bagging_freq': 6, 'lambda_l1': 0.04604986012546942, 'lambda_l2': 0.01507835805376649}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's auc: 0.853173\tvalid_1's auc: 0.817682\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.861565\tvalid_1's auc: 0.811973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:15:57,316]\u001b[0m Trial 92 finished with value: 0.8130151604356786 and parameters: {'num_leaves': 983, 'learning_rate': 0.08314812850765325, 'feature_fraction': 0.8916456044087423, 'bagging_fraction': 0.5981630738409466, 'bagging_freq': 6, 'lambda_l1': 0.018601467432051714, 'lambda_l2': 0.09439859545237006}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's auc: 0.85067\tvalid_1's auc: 0.813015\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.863987\tvalid_1's auc: 0.808264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:16:27,257]\u001b[0m Trial 93 finished with value: 0.812288416249632 and parameters: {'num_leaves': 953, 'learning_rate': 0.09298563936375362, 'feature_fraction': 0.9252448078342133, 'bagging_fraction': 0.6253291329748496, 'bagging_freq': 5, 'lambda_l1': 0.009235058341542597, 'lambda_l2': 0.0046191404571223225}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's auc: 0.850214\tvalid_1's auc: 0.812288\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.85195\tvalid_1's auc: 0.812162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:17:02,167]\u001b[0m Trial 94 finished with value: 0.8130450581395349 and parameters: {'num_leaves': 906, 'learning_rate': 0.05856320517907372, 'feature_fraction': 0.865911752076411, 'bagging_fraction': 0.5890105200156248, 'bagging_freq': 6, 'lambda_l1': 0.029056630180179416, 'lambda_l2': 0.035140465382642094}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's auc: 0.847905\tvalid_1's auc: 0.813045\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.862459\tvalid_1's auc: 0.807774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:17:29,248]\u001b[0m Trial 95 finished with value: 0.8100437886370326 and parameters: {'num_leaves': 865, 'learning_rate': 0.09978337469466167, 'feature_fraction': 0.8037902432065079, 'bagging_fraction': 0.6797211177579287, 'bagging_freq': 4, 'lambda_l1': 0.04977507745520909, 'lambda_l2': 0.01449657272892797}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's auc: 0.845488\tvalid_1's auc: 0.810044\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.857327\tvalid_1's auc: 0.812052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:18:07,720]\u001b[0m Trial 96 finished with value: 0.8122746173093905 and parameters: {'num_leaves': 981, 'learning_rate': 0.07328149920470728, 'feature_fraction': 0.8222068499102183, 'bagging_fraction': 0.5687250580936088, 'bagging_freq': 6, 'lambda_l1': 0.004976982301809624, 'lambda_l2': 0.000127319189830671}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.85702\tvalid_1's auc: 0.812275\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.850487\tvalid_1's auc: 0.812001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:18:44,702]\u001b[0m Trial 97 finished with value: 0.8127115837503679 and parameters: {'num_leaves': 932, 'learning_rate': 0.05218416441276048, 'feature_fraction': 0.8849794878480689, 'bagging_fraction': 0.6416916097351367, 'bagging_freq': 7, 'lambda_l1': 0.0006658177672186149, 'lambda_l2': 0.0007545731882641202}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.847928\tvalid_1's auc: 0.812712\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.855686\tvalid_1's auc: 0.809115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:19:14,181]\u001b[0m Trial 98 finished with value: 0.8102829702678833 and parameters: {'num_leaves': 954, 'learning_rate': 0.06372475278270784, 'feature_fraction': 0.8481696480130183, 'bagging_fraction': 0.7156361997859011, 'bagging_freq': 5, 'lambda_l1': 0.01196500399380784, 'lambda_l2': 0.002103932775730722}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's auc: 0.840986\tvalid_1's auc: 0.810283\n",
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.855744\tvalid_1's auc: 0.811191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 09:19:49,731]\u001b[0m Trial 99 finished with value: 0.8116421658816603 and parameters: {'num_leaves': 741, 'learning_rate': 0.08030394723489734, 'feature_fraction': 0.9085538880916514, 'bagging_fraction': 0.6178771155101888, 'bagging_freq': 3, 'lambda_l1': 0.09110952918569978, 'lambda_l2': 0.18003702154466145}. Best is trial 91 with value: 0.8176815020606417.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.855448\tvalid_1's auc: 0.811642\n",
      "Best Trial: score 0.8176815020606417,\n",
      "params {'num_leaves': 998, 'learning_rate': 0.0820671592743757, 'feature_fraction': 0.8930950799841506, 'bagging_fraction': 0.5944132149154198, 'bagging_freq': 6, 'lambda_l1': 0.04604986012546942, 'lambda_l2': 0.01507835805376649}\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04604986012546942, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04604986012546942\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8930950799841506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8930950799841506\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01507835805376649, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01507835805376649\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5944132149154198, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5944132149154198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.5944132149154198, bagging_freq=6,\n",
       "               feature_fraction=0.8930950799841506,\n",
       "               lambda_l1=0.04604986012546942, lambda_l2=0.01507835805376649,\n",
       "               learning_rate=0.0820671592743757, num_leaves=998)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(bagging_fraction=0.5944132149154198, bagging_freq=6,\n",
       "               feature_fraction=0.8930950799841506,\n",
       "               lambda_l1=0.04604986012546942, lambda_l2=0.01507835805376649,\n",
       "               learning_rate=0.0820671592743757, num_leaves=998)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.5944132149154198, bagging_freq=6,\n",
       "               feature_fraction=0.8930950799841506,\n",
       "               lambda_l1=0.04604986012546942, lambda_l2=0.01507835805376649,\n",
       "               learning_rate=0.0820671592743757, num_leaves=998)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "sampler = TPESampler(42)\n",
    "def objective(trial):\n",
    "    dtrain = lgb.Dataset(train[FEATS], y_train)\n",
    "    dtest = lgb.Dataset(test[FEATS], y_test)\n",
    "\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'seed': 42\n",
    "    }\n",
    "    model = lgb.train(\n",
    "        param, \n",
    "        dtrain,\n",
    "        valid_sets=[dtrain, dtest],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=1000,\n",
    "        early_stopping_rounds=100,\n",
    "    )\n",
    "\n",
    "    preds = model.predict(test[FEATS])\n",
    "    acc = accuracy_score(y_test, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    return auc\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler())\n",
    "study.optimize(objective,  n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "trial_params = trial.params\n",
    "print('Best Trial: score {},\\nparams {}'.format(trial.value, trial_params))\n",
    "\n",
    "# 최적의 파라미터로 모델 재학습\n",
    "final_lgb_model1 = lgb.LGBMClassifier(**trial_params)\n",
    "final_lgb_model1.fit(train[FEATS], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c048be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMClassifier(bagging_fraction=0.7319382550725074, bagging_freq=2,\n",
    "               feature_fraction=0.633004609254542,\n",
    "               lambda_l1=8.898067404913072e-08, lambda_l2=0.04345454720459056,\n",
    "               learning_rate=0.06524134688369941, num_leaves=626)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d86f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6088e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "test_df = pd.read_csv(DATA_PATH+'/test_data.csv')\n",
    "test_df[\"Timestamp\"] = test_df[\"Timestamp\"].apply(convert_time)\n",
    "test_df = feature_engineering(test_df)\n",
    "test_df = categorical_label_encoding(test_df, is_train=False) # LGBM을 위한 FE\n",
    "# test_df.to_csv(DATA_PATH + 'test_featured.csv', index=False)\n",
    "\n",
    "# Inference\n",
    "# test_df = pd.read_csv(DATA_PATH+'test_featured.csv')\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop(['answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "328c8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_lgb_model.predict(test_df[FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd1a8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAKE PREDICTION\n",
    "# predicts = np.mean(predicts_list, axis=0)\n",
    "\n",
    "submission = pd.read_csv(DATA_PATH+'/sample_submission.csv')\n",
    "submission['prediction'] = preds\n",
    "\n",
    "submission.to_csv(DATA_PATH+'/lgbm_kfold_tune_submission.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7448ed71",
   "metadata": {},
   "source": [
    "## probability로 출력하게 코드 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "test_df = pd.read_csv(DATA_PATH+'/test_data.csv')\n",
    "test_df[\"Timestamp\"] = test_df[\"Timestamp\"].apply(convert_time)\n",
    "test_df = feature_engineering(test_df)\n",
    "test_df = categorical_label_encoding(test_df, is_train=False) # LGBM을 위한 FE\n",
    "# test_df.to_csv(DATA_PATH + 'test_featured.csv', index=False)\n",
    "\n",
    "# Inference\n",
    "# test_df = pd.read_csv(DATA_PATH+'test_featured.csv')\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop(['answerCode'], axis=1)\n",
    "# # MAKE PREDICTION\n",
    "# predicts = np.mean(predicts_list, axis=0)\n",
    "​\n",
    "submission = pd.read_csv(DATA_PATH+'/sample_submission.csv')\n",
    "submission['prediction'] = preds\n",
    "​\n",
    "submission.to_csv(DATA_PATH+'/lgbm_pro_tune_auc_submission.csv',index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7723ed47",
   "metadata": {},
   "source": [
    "(bagging_fraction=0.5944132149154198, bagging_freq=6,\n",
    "feature_fraction=0.8930950799841506,\n",
    "lambda_l1=0.04604986012546942, lambda_l2=0.01507835805376649,\n",
    "learning_rate=0.0820671592743757, num_leaves=998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f668ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1188575, number of negative: 624388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1812963, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655598 -> initscore=0.643738\n",
      "[LightGBM] [Info] Start training from score 0.643738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.862076\tvalid_1's auc: 0.815763\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's auc: 0.853173\tvalid_1's auc: 0.817682\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(train[FEATS], y_train)\n",
    "dtest = lgb.Dataset(test[FEATS], y_test)\n",
    "param = {'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'bagging_fraction':0.5944132149154198, 'bagging_freq':6,\n",
    "        'feature_fraction':0.8930950799841506,\n",
    "        'lambda_l1':0.04604986012546942, 'lambda_l2':0.01507835805376649,\n",
    "        'learning_rate':0.0820671592743757, 'num_leaves':998,\n",
    "        'seed':42}\n",
    "\n",
    "model = lgb.train(\n",
    "    param, \n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dtest],\n",
    "    verbose_eval=100,\n",
    "    num_boost_round=1000,\n",
    "    early_stopping_rounds=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58f2c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_df[FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb876d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAKE PREDICTION\n",
    "# predicts = np.mean(predicts_list, axis=0)\n",
    "\n",
    "submission = pd.read_csv(DATA_PATH+'/sample_submission.csv')\n",
    "submission['prediction'] = preds\n",
    "\n",
    "submission.to_csv(DATA_PATH+'/lgbm_pro_tune_auc_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
